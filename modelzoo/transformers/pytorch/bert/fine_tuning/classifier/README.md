# PyTorch BERT fine-tuning classification model

- [PyTorch BERT fine-tuning classification model](#pytorch-bert-fine-tuning-classification-model)
  - [Model overview](#model-overview)
  - [Sequence of steps to perform](#sequence-of-steps-to-perform)
  - [Key features from CSoft platform used in this reference implementation](#key-features-from-csoft-platform-used-in-this-reference-implementation)
  - [Code structure](#code-structure)
  - [Dataset generation](#dataset-generation)
    - [Download](#download)
    - [Data preparation](#data-preparation)
    - [Input function description](#input-function-description)
      - [Features labels dictionary](#features-labels-dictionary)
  - [Compile model](#compile-model)
    - [Validate only](#validate-only)
    - [Compile only](#compile-only)
  - [Run fine-tuning](#run-fine-tuning)
    - [Run fine-tuning on Cerebras System](#run-fine-tuning-on-cerebras-system)
    - [Run fine-tuning on GPU](#run-fine-tuning-on-gpu)
  - [Configs included for this model](#Configs-included-for-this-model)
  - [References](#references)

## Model overview

This directory contains implementation of the BERT fine-tuning model for sequence (sentence) classification tasks. We present two flavors of the implementation:

1. Single sentence classification using [Stanford Sentiment Treebank (SST-2) dataset](https://nlp.stanford.edu/sentiment/index.html).
2. Sentence pair classification using [Multi-Genre Natural Language Inference (MNLI) dataset](https://cims.nyu.edu/~sbowman/multinli/).

This is a fine-tuning task, meaning, the BERT model [[1](#references)] is initialized with the weights generated by pre-training the model in a self-supervised fashion on the masked language modeling (MLM) and the next sentence prediction (NSP) tasks. During fine-tuning, the model architecture is adjusted by replacing MLM and NSP output heads with a classification head. The new output head is constructed by using two dense layers, feeding the `[CLS]` representation to a these layers for classification. The first dense layer is loaded from the pre-trained NSP head and the second dense layer is initialized with random weights.

Note: The TensorFlow version of this model is located at [TensorFlow Classifier](../../../../tf/bert/fine_tuning/classifier/).

## Sequence of steps to perform

The following block diagram shows a high-level view of the sequence of steps you will perform in this example.

<p align = "center">
<img src = ./images/steps-pt-classifier.png>
</p>
<p align = "center">
Fig.1 - Flow Chart of steps to fine-tune classification model
</p>

## Key features from CSoft platform used in this reference implementation

Fine-tuning classification model configs are supported in the [Layer Pipelined mode](https://docs.cerebras.net/en/latest/cerebras-basics/cerebras-execution-modes.html#layer-pipelined-mode
).

## Code structure

- `configs/`: YAML configuration files.
- `input/`: Input pipeline implementation for the above mentioned datasets.
- `model.py`: Model implementation leveraging [BertForSequenceClassification](../../bert_model.py) class.
- `data.py`: The entry point to the data input pipeline code.
- `run.py`: Training script. Performs training and validation.
- `utils.py`: Miscellaneous helper functions.

## Dataset generation

### Download

In order to download the raw dataset for [GLUE benchmark](https://gluebenchmark.com/tasks), download the dataset helper files from the GLUE benchmark [repository](https://github.com/nyu-mll/GLUE-baselines) by cloning it using the below command:

```bash
git clone https://github.com/nyu-mll/GLUE-baselines
```

To download the SST-2 and MNLI dataset, run the below command:

```bash
python download_glue_data.py --data_dir </path/to/data_dir> --tasks SST,MNLI
```

More information about the dataset and the script can be found on [GLUE Benchmark](https://openreview.net/pdf?id=rJ4km2R5t7).

### Data preparation

There is no separate data preparation step as we have designed the data input pipeline to use the downloaded `.tsv` files in the previous step. You can use the same path used in `--data_dir` flag above to populate the `data_dir` (for `train_input` and `eval_input`) field in the `yaml` config file.

### Input function description

The input to the model takes in either one sentence (`A`) or a pair of sentences (`A`, `B`). Sentence `A` is preceded by the `[CLS]` special token and is separated from sentence `B` by the `[SEP]` special token. The BERT model pre-training uses a special segment embedding layer at the input to signify the two sentences used in the NSP task. Here, that segment embedding is used to identify sentence `A` and a sentence `B` if it exists in the input. See [below](#features-labels-dictionary) for details.

The labels for this model indicate the class of the sentence to be classified for single sentence classification or the class indicating the relationship between the input sentence pair. This is supplied in the form of an integer corresponding the the class label.

If you want to use your own input function with this example code, then this section describes the input data format expected by `BertForSequenceClassificationModel` class defined in [model.py](./model.py). When you create your own custom BERT Classifier input function, you must ensure that your input function produces a features dictionary and a label tensor as described in this section.

#### Features labels dictionary

The input features and labels are passed to the model in one dictionary has the following key/values:

- `input_ids`: Input token IDs, padded with `0`s to `max_sequence_length`. The tokens in the dataset are mapped to these IDs using the vocabulary file. These values should be between `0` and `vocab_size - 1`, inclusive. The first token should be the special `[CLS]` token. The end of each sentence should be marked by the special `[SEP]` token. So, a sentence pair should be separated by additional `[SEP]` token.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `attention_mask`: Mask for padded positions. Has values `0` on the padded positions and `1` elsewhere.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `token_type_ids`: Segment IDs. A tensor the same size as the `input_ids` designating to which segment each token belongs. Each element of this tensor should only take the value `0` or `1`. The `[CLS]` token at the start, the question and the subsequent `[SEP]` token should all have the segment value `0`. The context and the subsequent `[SEP]` token should have the segment value `1`. All padding tokens after the last `[SEP]` should be in segment id `0`. Shown as `Segments` in the example below.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `labels`: Labels are integers per input sequence or sequence pairs corresponding to their classification labels.
  - Shape: `[batch_size, ]`
  - Type: `torch.int32`

An example of the input string and segment ID structure for single sentence classification:

``` bash
Tokens:   [CLS] The Matrix is a good movie. [SEP] [PAD] [PAD] ...
Segments:   0    0    0    0  0   0    0      0     0     0   ...
```

An example of the input string and segment ID structure for sentence pair classification:

``` bash
Tokens:   [CLS] The Matrix is a good movie. [SEP] So The Matrix Reloaded must also be good. [SEP] [PAD] [PAD] ...
Segments:   0    0    0    0  0   0    0      0   1   1     1      1      1    1   1    1     1     0     0   ...
```

NOTE: The input tokens are converted to IDs using the vocab file.

## Compile model

We recommend that you first compile your model successfully on a support cluster CPU node before running it on the CS system.

### Validate only

You can run in `validate_only` mode that runs a fast, light-weight verification. In this mode, the compilation will only run through the first few stages, up until kernel library matching. You can run it by using the below command:

```bash
csrun_cpu python-pt run.py \
  --mode=train \
  --params <path/to/yaml> \
  --model_dir </path/to/model_dir> \
  --validate_only 
```

After a successful `validate_only` run, you can run full compilation with `compile_only` mode.

### Compile only

This step runs the full compilation through all stages of the Cerebras software stack to generate a CS system executable. You can run it by using the below command:

```bash
csrun_cpu python-pt run.py \
  --mode=train \
  --params <path/to/yaml> \
  --model_dir </path/to/model_dir> \
  --compile_only 
```

When the above compilation is successful, the model is guaranteed to run on CS system. You can also use this mode to run pre-compilations of many different model configurations offline, so that you can fully utilize allotted CS system cluster time.

## Run fine-tuning

**IMPORTANT**: See the following notes before proceeding further.

**Parameter settings in YAML config file**: The config YAML files are located in the [configs](configs/) directory. Before starting a pre-training run, make sure that in the YAML config file you are using:

- The `train_input.data_dir` parameter points to the correct dataset, and
- The `train_input.max_sequence_length` parameter corresponds to the sequence length of the dataset.
- The `train_input.batch_size` parameter will set the batch size for the training.

Same applies for the `eval_input`.

**YAML config file differences**:

Please check [YAML config section](#yaml-config-file-description) for details on each config supported out of the box for this model.

### Run fine-tuning on Cerebras System

Follow [How to train on the CS System](../../../#how-to-train-on-the-cs-system) and execute the following command from within the Cerebras environment:

```bash
csrun_wse python-pt run.py \
--mode train \
--cs_ip x.x.x.x \
--params /path/to/yaml \
--model_dir /path/to/model_dir \
--checkpoint_path /path/to/pretrained_checkpoint.mdl
--is_pretrained_checkpoint
```

where:

- `/path/to/yaml` is a path to the YAML config file with model parameters. A few example YAML config files can be found in [configs](configs) directory.
- `/path/to/model_dir` is a path to the directory where you would like to store the logs and other artifacts of the run.
- `cs_ip` is the IP address of the CM endpoint.
- `checkpoint_path` is the path to the saved checkpoint from BERT pre-training.
- `is_pretrained_checkpoint` flag is needed for loading the pre-trained BERT model for fine-tuning.

### Run fine-tuning on GPU

To run pre-training on GPU, use the `run.py` Python utility as follows:

```bash
python run.py \
--mode train \
--params /path/to/yaml \
--model_dir /path/to/model_dir \
--checkpoint_path /path/to/pretrained_checkpoint.mdl
--is_pretrained_checkpoint
```

The description of the command-line arguments is the same as in the above section.

## Configs included for this model
Below is the list of yaml config files included for this model implementation at [configs](./configs/) folder:

- `bert_base_*.yaml` have the standard bert-base config with `hidden_size=768, num_hidden_layers=12, num_heads=12` as a backbone.
- `bert_large_*.yaml` have the standard bert-large config with `hidden_size=1024, num_hidden_layers=24, num_heads=16` as a backbone.

Additionally,

- `bert_*_sst2_*.yaml` corresponds to SST2 dataset config.
- `bert_*_mnli_*.yaml` corresponds to MNLI dataset config.

## References

[1] [BERT paper](https://arxiv.org/abs/1810.04805)
