train_input:
    data_processor : "T5DynamicDataProcessor"
    src_vocab_file: "./c4/vocab.txt" # https://huggingface.co/t5-small/resolve/main/spiece.model.
    src_data_dir: "./c4/en/train.tok.sentencepiece.3200"
    extra_ids: 100 # A number of extra ids added to the end of the vocabulary for use as sentinels.
    src_max_sequence_length: 512
    tgt_max_sequence_length: 114
    shuffle: True
    shuffle_seed: 1
    batch_size: 216 # this gives roughly 2**16 tokens per batch
                    # for GPU(16GB) set batch_size: 8
                    # 8 * 27 = 216 with grad_accum_steps: 27
    num_workers: 8
    prefetch_factor: 10
    persistent_workers: True
    dynamic_loss_weight: True

eval_input:
    data_processor : "T5DynamicDataProcessor"
    src_vocab_file: "./c4/vocab.txt" # https://huggingface.co/t5-small/resolve/main/spiece.model.
    src_data_dir: "./c4/en/validation.tok.sentencepiece.3200"
    extra_ids: 100 # A number of extra ids added to the end of the vocabulary for use as sentinels.
    src_max_sequence_length: 512
    tgt_max_sequence_length: 114
    shuffle: False
    shuffle_seed: 1 # also for deterministic masking
    batch_size: 216
    num_workers: 8
    prefetch_factor: 10
    persistent_workers: True

model:
    src_vocab_size: 32001
    extra_ids: 100

    ## Encoder
    encoder_num_hidden_layers: 12
    dropout_rate: 0.1

    # Encoder -- Attention
    d_kv: 64 # Size of the key, query, value projections per attention head.
    num_heads: 12 # d_kv * num_heads = hidden size.
    use_projection_bias_in_attention: False

    # Position Embeddings
    position_embedding_type: "relative"

    # Shared Weighed Embeddings
    share_embedding_weights: True
    share_encoder_decoder_embedding: True

    # T5 layer norm (no mean subtraction, and bias correction)
    use_t5_layer_norm: False

    # Encoder -- ffn
    d_ff: 3072 # Size of the intermediate feed forward layer in t5 blocks.
    d_model: 768  # Size of the encoder layers and the pooler layer.
    encoder_nonlinearity: "relu" # {"gelu", "relu", "gated-gelu"}
    decoder_nonlinearity: "relu" # {"gelu", "relu", "gated-gelu"}
    layer_norm_epsilon: 1.0e-6

    ## Decoder
    decoder_num_hidden_layers: 12

    # Loss scaling weight, 1/{average_number_valid_tokens}
    lm_loss_weight: 0.015

    # Cerebras configs.
    mixed_precision: True
    enable_vts: True

optimizer:
    optimizer_type: "AdamW" # in the paper AdaFactor is used, but AdamW should be a good alternative.
    correct_bias: True
    learning_rate:
        - scheduler: "Linear"
          initial_learning_rate: 0.0
          end_learning_rate: 0.0001
          steps: 10000
        - scheduler: "Linear"
          initial_learning_rate: 0.0001
          end_learning_rate: 0.00001
          steps: 514288
    loss_scaling_factor: "dynamic"
    weight_decay_rate: 0.01
    max_gradient_norm: 1.0
    # grad_accum_steps: 27 # helps fit in GPU memory

runconfig:
    max_steps: 524288
    log_steps: 100
    checkpoint_steps: 10000
    eval_steps: 2510
    seed: 1
    model_dir: "./model_dir"
    show_debug_metrics: False
    save_losses: True
