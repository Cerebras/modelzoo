# Transformer Large - according to Table 3.

### Input.
train_input:
    data_processor: TransformerDynamicDataProcessor
    src_data_dir: "./wmt16_en_de/train.tok.clean.bpe.32000.en"
    tgt_data_dir: "./wmt16_en_de/train.tok.clean.bpe.32000.de"
    src_vocab_file: "./wmt16_en_de/vocab.bpe.32000.en"
    tgt_vocab_file: "./wmt16_en_de/vocab.bpe.32000.de"
    src_max_sequence_length: 256
    tgt_max_sequence_length: 256
    batch_size: 2048
    num_buckets: 1
    repeat: True
    shuffle: True
    shuffle_buffer: 50000
    shuffle_seed: 12345
    use_multiple_workers: True

eval_input:
    data_processor: TransformerDynamicDataProcessor
    src_data_dir: "./wmt16_en_de/newstest2014.tok.clean.bpe.32000.en"
    tgt_data_dir: "./wmt16_en_de/newstest2014.tok.clean.bpe.32000.de"
    src_vocab_file: "./wmt16_en_de/vocab.bpe.32000.en"
    tgt_vocab_file: "./wmt16_en_de/vocab.bpe.32000.de"
    src_max_sequence_length: 256
    tgt_max_sequence_length: 256
    batch_size: 16
    num_buckets: 1

model:
    # Embedding.
    share_encoder_decoder_embedding: True
    position_embedding_type: "fixed"  # valid options: "learned", "fixed"
    hidden_size: 1024 # d_model

    # Encoder/Decoder.
    encoder_num_hidden_layers: 6
    decoder_num_hidden_layers: 6
    dropout_rate: 0.3
    layer_norm_epsilon: 1e-5

    # Encoder/Decoder Attention.
    num_heads: 16
    attention_type: "scaled_dot_product"  # valid options: "dot_product", "scaled_dot_product"
    attention_dropout_rate: 0.1
    use_projection_bias_in_attention: False
    use_ffn_bias_in_attention: False

    # Encoder/Decoder - ffn.
    filter_size: 4096  # d_ff
    encoder_nonlinearity: "relu"
    decoder_nonlinearity: "relu"
    use_ffn_bias: True

    # Cerebras configs.
    dropout_seed: 0
    weight_initialization_seed: 0
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    use_vsl: True


### Optimization.
optimizer:
    optimizer_type: "adam"
    beta1: 0.9
    beta2: 0.98
    epsilon: 1.0e-6
    disable_lr_steps_reset: False
    learning_rate:
      - end_learning_rate: 0.000698683912937353
        initial_learning_rate: 1.0e-07
        scheduler: Linear
        steps: 4000
      - end_learning_rate: 0.00037894798246424215
        initial_learning_rate: 0.000698683912937353
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00029014271289331547
        initial_learning_rate: 0.00037894798246424215
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00024401778319601815
        initial_learning_rate: 0.00029014271289331547
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00021462335024901533
        initial_learning_rate: 0.00024401778319601815
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00019380240931989222
        initial_learning_rate: 0.00021462335024901533
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00017806195541597027
        initial_learning_rate: 0.00019380240931989222
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00016562350566866137
        initial_learning_rate: 0.00017806195541597027
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00015547359888418554
        initial_learning_rate: 0.00016562350566866137
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00014698682270697125
        initial_learning_rate: 0.00015547359888418554
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00013975354982773464
        initial_learning_rate: 0.00014698682270697125
        scheduler: Linear
        steps: 9600
    loss_scaling_factor: "dynamic"
    log_summaries: True

### Cerebras parameters.
runconfig:
    max_steps: 300000
    eval_steps: 187
    save_summary_steps: 100
    save_checkpoints_steps: 10000
    keep_checkpoint_max: 5
    tf_random_seed: 1202
    enable_distributed: False

### CS-specific configurations.
csconfig:
    use_cbfloat16: False
