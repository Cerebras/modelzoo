#Linformer with RoBERTa-Base trained for ~23 epochs on WikiCorpus, projected_dims: 256, shared-heads

### Input
train_input:
  data_processor: "BertMlmOnlyTfRecordsDynamicMaskProcessor"
  data_dir: "./language/datasets/wikicorpus/train_cased_msl512_mlm_only_unmasked"
  vocab_file: "../../vocab/google_research_uncased_L-12_H-768_A-12.txt"
  do_lower: False
  max_sequence_length: 512
  max_predictions_per_seq: 80
  mask_whole_word: False
  shuffle: True
  batch_size: 1024

model:
  disable_nsp: True
  # Embedding
  hidden_size: 768
  use_position_embedding: True
  use_segment_embedding: False
  position_embedding_type: "learned" # {"learned", "fixed"}
  max_position_embeddings: 512
  share_embedding_weights: True

  # Encoder
  num_hidden_layers: 12
  dropout_rate: 0.1
  layer_norm_epsilon: 1e-5

  # Encoder - Attention
  num_heads: 12
  attention_type: "scaled_dot_product" # {"dot_product", "scaled_dot_product"}
  attention_dropout_rate: 0.1
  use_projection_bias_in_attention: True
  use_ffn_bias_in_attention: True

  # LinFormer specific
  projected_dims: 256
  attention_style: "linformer-shared-heads" # {"linformer", "linformer-shared-heads", "linformer-shared-kv"}
  disable_attention: False 
  # If disable_attention=True, replaces the attention computation i.e softmax(QK^T)V and all that follows 
  # with a elementwise multiply of Q and Q
  # Note that the initial projection of K, V matrices specific to Linformer will be computed.
  # This flag is only for DTG debugging purposes.

  # Encoder - ffn
  filter_size: 3072
  encoder_nonlinearity: "gelu"
  use_ffn_bias: True

  # Task-specific
  use_ffn_bias_in_mlm: True
  use_output_bias_in_mlm: True
  mlm_nonlinearity: "gelu"
  mlm_loss_scaling: "precomputed_num_masked" # {"num_masked", "batch_size", "precomputed_num_masked"}
  mlm_loss_weight: 1 

  dropout_seed: 0
  weight_initialization_seed: 0
  mixed_precision: True
  boundary_casting: False
  tf_summary: False

### Optimization
# Section 5.2: All of our models,
# including the Transformer baselines, were pretrained with the same objective, pretraining corpus, and
# up to 250k updates (although our Linformer takes much less wall-clock time to get to 250k updates,
# and was consequently trained for less time).
optimizer:
  optimizer_type: "adamw" # {"sgd", "momentum", "adam", "adamw"}
  weight_decay_rate: 0.01
  epsilon: 1e-6
  max_gradient_norm: 1.0
  disable_lr_steps_reset: True
  learning_rate:
    - steps: 25000
      scheduler: "Linear"
      initial_learning_rate: 0.0
      end_learning_rate: 0.0001
    - scheduler: "Linear"
      initial_learning_rate: 0.0001
      end_learning_rate: 0.0
      steps: 250000
  loss_scaling_factor: "dynamic"
  log_summaries: True

### Cerebras parameters
runconfig:
  max_steps: 250000 # 22.88 epochs over 11186076 WikiCorpus examples
  save_summary_steps: 100
  save_checkpoints_steps: 10000
  keep_checkpoint_max: 0
  tf_random_seed: 1202
  enable_distributed: False

### CS-specific configurations
csconfig:
  use_cbfloat16: False
