train_input:
    data_processor: "GenomicDataProcessor"
    data_dir: "./anl_genomic/projects/RL-fold/sarscov2_filtered/train"
    vocab_file: "./anl_genomic/Gene_vocab_codon.txt"
    add_special_tokens: False
    batch_size: 33
    max_sequence_length: 10240
    mixed_precision: true
    shuffle: false
    vocab_size: 70
    n_gram: 3

eval_input:
    data_processor: "GptHDF5DataProcessor"
    data_dir: "./owt_pretraining_gpt_hdf5/val_msl2048"
    max_sequence_length: 2048
    batch_size: 112 # 8, if run on 4 GPUs
    shuffle: False
    num_workers: 8

### Model
model:
    # Embedding
    hidden_size: 768
    vocab_size: 70
    use_position_embedding: True
    position_embedding_type: "learned"
    share_embedding_weights: True
    max_position_embeddings: 10240

    # Encoder
    num_hidden_layers: 12
    dropout_rate: 0.0
    layer_norm_epsilon: 1.0e-5 
    loss_weight: 0.00009765625 # 1/MSL # 0.00048828125 

    # Encoder - Attention
    num_heads: 12
    attention_type: "scaled_dot_product"
    attention_dropout_rate: 0.0
    use_projection_bias_in_attention: True
    # use_ffn_bias_in_attention: True

    # Encoder - ffn
    filter_size: 3072
    nonlinearity: "gelu"
    use_ffn_bias: True

    # Task-specific
    # use_bias_in_output: False

    # Cerebras parameters
    mixed_precision: True


### Optimization
optimizer:
    optimizer_type: "adamw"
    epsilon: 1.0e-5
    max_gradient_norm: 1.0
    learning_rate: 0.00028
    loss_scaling_factor: "dynamic"  # Change to "tf_dynamic" on GPU
    log_summaries: True

### Cerebras parameters
runconfig:
    max_steps: 1000
    log_steps: 5
    checkpoint_steps: 50
    save_initial_checkpoint: False
    save_losses: True
    eval_steps: 32
