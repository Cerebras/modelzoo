train_input:
  data_processor: "GenomicDataProcessor"
  data_dir: "./anl_genomic/projects/RL-fold/sarscov2_filtered/train"
  vocab_file: "./anl_genomic/Gene_vocab_codon.txt"
  vocab_size: 70
  max_sequence_length: 10240
  add_special_tokens: false
  shuffle: True
  repeat: True
  batch_size: 6 
  shuffle_seed: 256
  n_gram: 3

eval_input:
  data_processor: "GenomicDataProcessor"
  data_dir: "./anl_genomic/projects/RL-fold/sarscov2_filtered/val" 
  add_special_tokens: False
  vocab_size: 70
  max_sequence_length: 10240
  batch_size: 6 
  repeat: false
  shuffle: false

model:
  share_embedding_weights: True
  max_position_embeddings: 10240

  hidden_size: 8064
  num_heads: 64
  num_hidden_layers: 64

  use_projection_bias_in_attention: True
  use_ffn_bias_in_attention: True
  use_ffn_bias: True

  filter_size: 8000
  nonlinearity: "gelu"

  rotary_dim: 64
  layer_norm_epsilon: 1.0e-5
  use_bias_in_output: False

  embedding_initializer:
    - name: "scaled_init_normal"
      key: "vocab_size"

  initializer:
    - name: "variance_scaling"
      scale: 1.0

  output_layer_initializer:
    - name: "variance_scaling"
      scale_type: "wang_init"

  mixed_precision: True
  boundary_casting: False
  tf_summary: False

optimizer:
  optimizer_type: "adamw"
  epsilon: 1.0e-5 
  weight_decay_rate: 0.01 
  max_gradient_norm: 1.0
  use_bias_correction: True
  max_loss_scale: 2147483648.0 
  learning_rate:
    - scheduler: "Linear"
      initial_learning_rate: 0.0
      end_learning_rate: 4.3e-5
      steps: 23630 # approx 1.5M samples
    - scheduler: "Cosine"
      initial_learning_rate: 4.3e-5
      alpha: 0.1
      decay_steps: 2363000 # approx 150M samples
    - scheduler: "Constant"
      learning_rate: 4.3e-6
  loss_scaling_factor: "dynamic"

runconfig:
  max_steps: 200 # 2700000 # approx 175M samples
  save_summary_steps: 10
  save_checkpoints_steps: 200
  keep_checkpoint_max: 1
  enable_distributed: False
