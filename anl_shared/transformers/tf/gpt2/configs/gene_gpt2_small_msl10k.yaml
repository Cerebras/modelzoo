# This is a Gene Transformer configuration.
#
# The model architecture should replicate ModelZoo standard GPT2 small,
# except the parameters relevant to sequence length, such as max_sequence_length in train_input
# and eval_input, and max_position_embeddings in model.
#
# The other major differences lie in parameters that concern the input data,
# such as train_input and eval_input parameters (data_processor, data_dir, vocab_file, 
# vocab_size, n_gram).


train_input:
  data_processor: "GenomicDataProcessor"
  data_dir: "./anl_genomic/projects/RL-fold/sarscov2_filtered/train"
  vocab_file: "./anl_genomic/Gene_vocab_codon.txt"
  vocab_size: 70
  max_sequence_length: 10240
  add_special_tokens: false
  shuffle: false
  repeat: true
  batch_size: 33
  mixed_precision: true
  skip_steps: 0
  n_gram: 3
  
eval_input:
  data_processor: "GenomicDataProcessor"
  data_dir: "./anl_genomic/projects/RL-fold/sarscov2_filtered/val" 
  add_special_tokens: false
  vocab_size: 70
  max_sequence_length: 10240
  batch_size: 6
  repeat: false
  shuffle: false
  n_gram: 3
  
  
model:

  # Embedding
  hidden_size: 768
  use_position_embedding: true
  position_embedding_type: learned
  share_embedding_weights: true
  max_position_embeddings: 10240
  
  # Encoder
  num_hidden_layers: 12
  dropout_rate: 0.0
  layer_norm_epsilon: 1.0e-05
  
  # Encoder - Attention
  num_heads: 12
  attention_dropout_rate: 0.0
  use_projection_bias_in_attention: true
  attention_type: scaled_dot_product
  
  # Encoder - ffn
  filter_size: 3072
  nonlinearity: gelu
  use_ffn_bias: true
  use_ffn_bias_in_attention: true
  
  # task specific
  loss_scaling: num_tokens
  loss_weight: 1.0
  
  # cerebras specific
  mixed_precision: true
  boundary_casting: false
  tf_summary: false
  
  
  # others
  bits_per_x_dataset: openwebtext2
  disable_attention_op: false
  dropout_seed: null
  embedding_initializer: null
  embedding_size: 768
  fixed_sparse_attention: null
  initializer:
    mean: 0.0
    name: truncated_normal
    stddev: 0.02
  input_pad_id: -1
  output_layer_initializer:
    mean: 0.0
    name: truncated_normal
    stddev: 0.004082482904638631
  output_nonlinearity: null
  topk_args: null
  use_bias_in_output: false
  use_pre_normalization: true
  weight_initialization_seed: 0
  
optimizer:
  optimizer_type: adamw
  epsilon: 1.0e-05
  grad_accum_steps: 1
  learning_rate: 0.00028
  log_summaries: true
  loss_scaling_factor: dynamic
  max_gradient_norm: 1.0
  
  
runconfig:
  disable_standard_logs: false
  enable_distributed: false
  keep_checkpoint_max: 100
  log_step_count_steps: 50
  max_steps: 1000
  multiple_workers: false
  save_checkpoints_steps: 10
  save_summary_steps: 5
  tf_random_seed: 0
  throttle_secs: 0
