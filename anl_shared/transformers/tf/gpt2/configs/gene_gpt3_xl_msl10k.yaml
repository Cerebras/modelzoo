# This is a Gene Transformer configuration.
#
# The model architecture should replicate ModelZoo standard GPT3 XLarge,
# except the parameters relevant to sequence length, such as max_sequence_length in train_input
# and eval_input, and max_position_embeddings in model.
#
# The other major differences lie in parameters that concern the input data,
# such as train_input and eval_input parameters (data_processor, data_dir, vocab_file, 
# vocab_size, n_gram).

train_input:
  data_processor: "GenomicDataProcessor"
  batch_size: 22 
  data_dir: "./anl_genomic/projects/RL-fold/sarscov2_filtered/train"
  vocab_file: "./anl_genomic/Gene_vocab_codon.txt"
  vocab_size: 70
  max_sequence_length: 10240
  add_special_tokens: false
  repeat: true
  shuffle: true
  shuffle_seed: 256
  use_multiple_workers: false
  n_gram: 3
  
eval_input:
  data_processor: "GenomicDataProcessor"
  data_dir: "./anl_genomic/projects/RL-fold/sarscov2_filtered/val" 
  add_special_tokens: false
  vocab_size: 70
  max_sequence_length: 10240
  batch_size: 6
  repeat: false
  shuffle: false
  n_gram: 3
  
model:

  # Embedding
  hidden_size: 2048
  use_position_embedding: true
  position_embedding_type: learned
  share_embedding_weights: true
  max_position_embeddings: 10240
  
  # Encoder
  num_hidden_layers: 24
  dropout_rate: 0.0
  layer_norm_epsilon: 1.0e-05
  
  # Encoder - Attention
  num_heads: 16
  attention_dropout_rate: 0.0
  use_projection_bias_in_attention: true
  attention_type: scaled_dot_product
  
  # Encoder - ffn
  filter_size: 8192
  nonlinearity: gelu
  use_ffn_bias: true
  use_ffn_bias_in_attention: true
  
  # task specific 
  loss_scaling: "num_tokens" 
  use_bias_in_output: false
  
  # Cerebras parameters 
  boundary_casting: false
  mixed_precision: true
  tf_summary: false
  
  # others
  weight_initialization_seed: 0
  
optimizer:
  optimizer_type: adamw
  weight_decay_rate: 0.01
  max_gradient_norm: 1.0
  learning_rate: 0.0003
  loss_scaling_factor: dynamic
  log_summaries: true
  
  # others
  epsilon: 1.0e-05
  max_loss_scale: 2147483648.0
  use_bias_correction: true
  ws_summary: true
  
runconfig:
  max_steps: 10000
  save_summary_steps: 10
  tf_random_seed: 0
  save_checkpoints_steps: 50
  keep_checkpoint_max: 0
