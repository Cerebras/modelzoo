# Transformer Base - according to Table 3.

### Input.
train_input:
    data_processor: STFDataProcessor
    data_path: "./data/temperature-v1_train.csv"
    batch_size: 1024
    num_buckets: 1
    repeat: True
    shuffle: True
    shuffle_seed: 12345
    use_multiple_workers: True
    time_resolution: 1
    context_length: 160
    target_length: 40
    start_token_length: 8
    num_vars: 6
    contains_hr_min: False

eval_input:
    data_processor: STFDataProcessor
    data_path: "./data/temperature-v1_val.csv"
    scaler_stats_path: "./data/temperature-v1_scaler_stats.json"
    batch_size: 256
    num_buckets: 1
    time_resolution: 1
    context_length: 160
    target_length: 40
    start_token_length: 8
    num_vars: 6
    contains_hr_min: False
    repeat: True
    shuffle: False

model:
    # Embedding.
    share_encoder_decoder_embedding: True
    position_embedding_type: "fixed"  # valid options:  "learned", "fixed"
    hidden_size: 256 # d_model
    
    # Encoder/Decoder.
    encoder_num_hidden_layers: 6
    decoder_num_hidden_layers: 6
    dropout_rate: 0.1
    layer_norm_epsilon: 1.0e-5

    # Encoder/Decoder Attention.
    num_heads: 8
    attention_type: "scaled_dot_product"  # valid options: "dot_product", "scaled_dot_product"
    attention_dropout_rate: 0.1
    use_projection_bias_in_attention: False 
    use_ffn_bias_in_attention: False 

    # Encoder/Decoder - ffn.
    filter_size: 1024  # d_ff
    encoder_nonlinearity: "relu"
    decoder_nonlinearity: "relu"
    use_ffn_bias: True

    # Cerebras configs.
    dropout_seed: 0
    weight_initialization_seed: 0
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    

### Optimization.
optimizer:
    optimizer_type: "adamw"
    beta1: 0.9
    beta2: 0.98
    weight_decay_rate: 0.0
    epsilon: 1.0e-6
    disable_lr_steps_reset: True
    learning_rate:
      - scheduler: "Linear"
        initial_learning_rate: 1.0e-7
        end_learning_rate: 1.0e-5
        steps: 3000
      - scheduler: "Constant"
        learning_rate: 1.0e-5
    loss_scaling_factor: "dynamic"
    log_summaries: True
    

### Cerebras parameters.
runconfig:
    max_steps: 500000
    save_summary_steps: 100 
    save_checkpoints_steps: 5000
    keep_checkpoint_max: 100
    # tf_random_seed: 1202
    enable_distributed: False

### CS-specific configurations.
csconfig:
    use_cbfloat16: False
