#BERT-Base

### Input
train_input:
    data_processor: SecondaryStructureDataProcessor
    data_dir: "./data/secondary_structure/train/"
    max_sequence_length: 257
    add_special_tokens: True
    batch_size: 32
    shuffle: True
    num_classes: 8 # until release 1.0, must be even

eval_input:
    data_processor: SecondaryStructureDataProcessor
    data_dir: "./data/secondary_structure/eval"
    max_sequence_length: 257
    add_special_tokens: True
    batch_size: 32
    num_classes: 8 # until release 1.0, must be even

model:
    pretrain_params_path: "../../configs/params_tape_bert_base.yaml"
    encoder_output_dropout_rate: 0.1
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    num_classes: 8 # until release 1.0, must be even
    loss_weight: 0.0039 # `loss_weight` allows us to apply scaling of the loss
                        # not just across the `batch_size`, but also
                        # across number of non padded tokens with this formula
                        # `loss_weight` = 1 / {average number of non padded tokens per batch}
                        # Scaling by the `batch_size` is provided in the model code as well.

### Optimization
optimizer:
    optimizer_type: "adamw" # {"sgd", "momentum", "adam", "adamw"}
    weight_decay_rate: 0.01
    epsilon: 1e-6
    max_gradient_norm: 1.0
    learning_rate:
        - scheduler: "Linear"
          steps: 100
          initial_learning_rate: 0.0
          end_learning_rate: 0.0001
        - scheduler: "Linear"
          initial_learning_rate: 0.0001
          end_learning_rate: 0.0
    loss_scaling_factor: "dynamic"

### Cerebras parameters
runconfig:
    max_steps: 2700 # 10 epochs
    save_summary_steps: 100
    save_checkpoints_steps: 1000
    eval_steps: 0 # eval_steps <= 0 means we use all of the eval data
    keep_checkpoint_max: 2
    tf_random_seed: 1202
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    throttle_secs: 0
