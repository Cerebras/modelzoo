# This file is generated by create_dinov2_config_with_schedulers.py
# --input_file_name: params_dinov2_large_patch16_img224_cszoov2.yaml
#
# This is the training config for pre-training DINOv2 large model with 224x224 
# input size and batch size 2048 on ImageNet-1K dataset.
# Trains for a total of 100 epochs with a cosine learning rate schedule.
# Applies per layer decay based on the decay_rate: 0.9 and patch embed multiplier: 0.2
trainer:
  init:
    seed: 1
    backend:
      backend_type: CSX
      cluster_config:
        num_csx: 1
        # Increase workers for more performance if there are more worker nodes.
        num_workers_per_csx: 2
    model:
      name: dino
      image_model_trunks:
      - image_model:
          name: MultiImageViTModel
          position_embedding_type: learned
          embedding_dropout_rate: 0.0
          hidden_size: 1024
          use_post_embed_layer_norm: false
          use_embed_proj_bias: true
          interpolate_position_embedding: &id001
            antialias: false
            interpolate_offset: 0.1
            local_patch_dims:
            - 6
            - 6
          projection_initializer: &id002
            name: kaiming_uniform
            a: 2.2360679775
          position_embedding_initializer: &id003
            name: truncated_normal
            std: 0.02
            a: -2.0
            b: 2.0
          cls_token_initializer: &id004
            name: normal
            std: 1.0e-06
          default_initializer: &id005
            name: truncated_normal
            std: 0.02
            a: -2.0
            b: 2.0
          num_hidden_layers: 24
          layer_norm_epsilon: 1.0e-06
          layerscale_value: 0.01
          num_heads: 16
          attention_type: scaled_dot_product
          attention_softmax_fp32: true
          dropout_rate: 0.0
          nonlinearity: gelu
          attention_dropout_rate: 0.0
          use_projection_bias_in_attention: true
          use_ffn_bias_in_attention: true
          filter_size: 4096
          use_ffn_bias: true
          initializer_range: 0.02
          norm_first: true
          image_size: &id006
          - 224
          - 224
          num_channels: 3
          patch_size: &id007
          - 16
          - 16
          use_conv_patchified_embedding: false
          use_encoder_pooler_layer: false
          prepend_cls_token: true
          use_masked_patches: true
        forward_args:
        - - ssl_transform.output('global_view')
          - null
          - null
        stop_grad: true
      - image_model:
          name: MultiImageViTModel
          position_embedding_type: learned
          embedding_dropout_rate: 0.0
          hidden_size: 1024
          use_post_embed_layer_norm: false
          use_embed_proj_bias: true
          interpolate_position_embedding: *id001
          projection_initializer: *id002
          position_embedding_initializer: *id003
          cls_token_initializer: *id004
          default_initializer: *id005
          num_hidden_layers: 24
          layer_norm_epsilon: 1.0e-06
          layerscale_value: 0.01
          num_heads: 16
          attention_type: scaled_dot_product
          attention_softmax_fp32: true
          dropout_rate: 0.0
          nonlinearity: gelu
          attention_dropout_rate: 0.0
          use_projection_bias_in_attention: true
          use_ffn_bias_in_attention: true
          filter_size: 4096
          use_ffn_bias: true
          initializer_range: 0.02
          norm_first: true
          image_size: *id006
          num_channels: 3
          patch_size: *id007
          use_conv_patchified_embedding: false
          use_encoder_pooler_layer: false
          prepend_cls_token: true
          use_masked_patches: true
          stochastic_depth_drop_prob: 0.3
          stochastic_depth_mode: row
          stochastic_depth_drop_prob_schedule: constant
        forward_args:
        - - ssl_transform.output('global_view')
          - null
          - ssl_transform.output('collated_masks')
        - - ssl_transform.output('local_view')
          - null
          - null
        stop_grad: false
      heads:
      - head_model: &id008
          name: DinoHead
          input_size: 1024
          hidden_size: 2048
          output_size: 65536
          bottleneck_size: 256
          norm_last_layer: false
          num_layers: 3
          initializer:
            name: truncated_normal
            std: 0.02
        forward_args:
        - - image_model_trunks.output(0,0,1)
        - - image_model_trunks.output(0,0,0)
        stop_grad: true
      - head_model: *id008
        forward_args:
        - - image_model_trunks.output(1,0,1)
        - - image_model_trunks.output(1,1,1)
        - - image_model_trunks.output(1,0,0)
        stop_grad: false
      losses:
      - loss:
          name: DinoDistillationLoss
          input_size: 65536
          warmup_teacher_temp: 0.04
          teacher_temp: 0.07
          warmup_teacher_temp_steps: 37500
          total_steps: 125000
          student_temp: 0.1
          center_momentum: 0.9
          teacher_temp_scheduler: linear_cosine
        forward_args:
        - - heads.output(1,0,0)
          - heads.output(0,0,0)
          - heads.output(1,1,0)
        stop_grad: false
      - loss:
          name: iBOTPatchLoss
          input_size: 65536
          warmup_teacher_temp: 0.04
          teacher_temp: 0.07
          warmup_teacher_temp_steps: 37500
          total_steps: 125000
          student_temp: 0.1
          center_momentum: 0.9
          teacher_temp_scheduler: linear_cosine
        forward_args:
        - - heads.output(0,1,0)
          - heads.output(1,2,0)
          - ssl_transform.output('collated_masks')
        stop_grad: false
      copy_init_weights:
      - source: image_model_trunks.model(0)
        target: image_model_trunks.model(1)
      - source: heads.model(0)
        target: heads.model(1)
      ema:
      - source: image_model_trunks.model(1)
        target: image_model_trunks.model(0)
        scheduler_name: cosine
        scheduler_params:
          ema_decay_start: 0.992
          ema_decay_end: 1.0
          total_steps: 125000
      - source: heads.model(1)
        target: heads.model(0)
        scheduler_name: cosine
        scheduler_params:
          ema_decay_start: 0.992
          ema_decay_end: 1.0
          total_steps: 125000
      freeze:
      - image_model_trunks.model.0.*
      - heads.model.0.*
    optimizer:
      AdamW:
        betas:
        - 0.9
        - 0.999
        eps: 1.0e-08
        correct_bias: true
        params:
        - params:
          - '*.mask_token'
          - '*.cls_embedding'
          - '*.embedding_layer.*.weight'
          - '*.mlp.*.weight'
          - '*.last_layer.weight*'
          - '*.encoder.*.self_attn.*.weight'
          - '*.encoder.*.ffn.*.weight'
          tag: wd_params
        - params:
          - '*position_embeddings*'
          - '*.mask_token*'
          - '*.cls_embedding*'
          tag: lr_emb_params
        - params:
          - '*.linear_proj.*'
          tag: lr_proj_params
        - params:
          - '*last_layer.*'
          tag: lr_last_layer_params
        - params:
          - '*.mlp.*'
          - '*.transformer_encoder.norm.*'
          tag: default_lr_params
        - params:
          - '*.transformer_encoder.layers.0.*'
          tag: lr_layer_0_params
        - params:
          - '*.transformer_encoder.layers.1.*'
          tag: lr_layer_1_params
        - params:
          - '*.transformer_encoder.layers.2.*'
          tag: lr_layer_2_params
        - params:
          - '*.transformer_encoder.layers.3.*'
          tag: lr_layer_3_params
        - params:
          - '*.transformer_encoder.layers.4.*'
          tag: lr_layer_4_params
        - params:
          - '*.transformer_encoder.layers.5.*'
          tag: lr_layer_5_params
        - params:
          - '*.transformer_encoder.layers.6.*'
          tag: lr_layer_6_params
        - params:
          - '*.transformer_encoder.layers.7.*'
          tag: lr_layer_7_params
        - params:
          - '*.transformer_encoder.layers.8.*'
          tag: lr_layer_8_params
        - params:
          - '*.transformer_encoder.layers.9.*'
          tag: lr_layer_9_params
        - params:
          - '*.transformer_encoder.layers.10.*'
          tag: lr_layer_10_params
        - params:
          - '*.transformer_encoder.layers.11.*'
          tag: lr_layer_11_params
        - params:
          - '*.transformer_encoder.layers.12.*'
          tag: lr_layer_12_params
        - params:
          - '*.transformer_encoder.layers.13.*'
          tag: lr_layer_13_params
        - params:
          - '*.transformer_encoder.layers.14.*'
          tag: lr_layer_14_params
        - params:
          - '*.transformer_encoder.layers.15.*'
          tag: lr_layer_15_params
        - params:
          - '*.transformer_encoder.layers.16.*'
          tag: lr_layer_16_params
        - params:
          - '*.transformer_encoder.layers.17.*'
          tag: lr_layer_17_params
        - params:
          - '*.transformer_encoder.layers.18.*'
          tag: lr_layer_18_params
        - params:
          - '*.transformer_encoder.layers.19.*'
          tag: lr_layer_19_params
        - params:
          - '*.transformer_encoder.layers.20.*'
          tag: lr_layer_20_params
        - params:
          - '*.transformer_encoder.layers.21.*'
          tag: lr_layer_21_params
        - params:
          - '*.transformer_encoder.layers.22.*'
          tag: lr_layer_22_params
        - params:
          - '*.transformer_encoder.layers.23.*'
          tag: lr_layer_23_params
    precision:
      enabled: true
      fp16_type: cbfloat16
      loss_scaling_factor: dynamic
      max_gradient_norm: 1.0
      precision_opt_level: 1
      log_loss_scale: true
    loop:
      max_steps: 125000
    logging:
      log_steps: 10
    checkpoint:
      checkpoint_name: checkpoint_{step}.mdl
      steps: 2000
    callbacks:
    - ScopedTrainFlags:
        csx.performance.micro_batch_size: 256
    - KeepNCheckpoints:
        n: 2
    - ComputeNorm: {}
    schedulers:
    - CosineDecayWD:
        initial_val: 0.04
        end_val: 0.4
        total_iters: 125000
        param_group_tags: wd_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.00040610442823766876
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.00040610442823766876
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_emb_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 8.122088564753376e-05
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 8.122088564753376e-05
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_proj_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0
            total_iters: 1250
        - LinearLR:
            initial_learning_rate: 0.0005656854249492381
            end_learning_rate: 0.005656854249492381
            total_iters: 11250
        - CosineDecayLR:
            initial_learning_rate: 0.005656854249492381
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_last_layer_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.005656854249492381
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.005656854249492381
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: default_lr_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.00045122714248629865
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.00045122714248629865
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_0_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.000501363491651443
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.000501363491651443
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_1_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.000557070546279381
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.000557070546279381
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_2_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0006189672736437566
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0006189672736437566
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_3_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0006877414151597296
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0006877414151597296
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_4_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0007641571279552551
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0007641571279552551
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_5_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.000849063475505839
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.000849063475505839
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_6_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0009434038616731545
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0009434038616731545
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_7_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0010482265129701715
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0010482265129701715
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_8_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0011646961255224128
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0011646961255224128
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_9_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0012941068061360142
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0012941068061360142
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_10_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0014378964512622378
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0014378964512622378
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_11_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.001597662723624709
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.001597662723624709
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_12_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0017751808040274539
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0017751808040274539
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_13_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0019724231155860603
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0019724231155860603
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_14_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0021915812395400668
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0021915812395400668
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_15_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0024350902661556297
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0024350902661556297
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_16_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.002705655851284033
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.002705655851284033
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_17_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.003006284279204481
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.003006284279204481
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_18_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0033403158657827566
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0033403158657827566
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_19_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0037114620730919513
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.0037114620730919513
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_20_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.004123846747879946
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.004123846747879946
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_21_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.004582051942088829
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.004582051942088829
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_22_params
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.005091168824543143
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.005091168824543143
            end_learning_rate: 1.0e-06
            total_iters: 112500
        param_group_tags: lr_layer_23_params
  fit:
    train_dataloader:
      data_processor: DinoImageDataProcessor
      batch_size: 2048
      shuffle: true
      shuffle_seed: 1456354
      num_workers: 4
      prefetch_factor: 4
      dataset:
        name: Imagenet
        root: ./computer_vision/datasets/imagenet/imagenet1k_ilsvrc2012
        split: train
      ssl_transform:
        name: Dinov2Transform
        multi_crop_transform:
          global_num_crops: 2
          local_num_crops: 8
          global_image_size: 224
          local_image_size: 96
          global_crops_scale:
          - 0.32
          - 1
          local_crops_scale:
          - 0.05
          - 0.32
          multicrop_transform_list:
          - name: random_horizontal_flip
            p: 0.5
          - name: color_jitter_with_prob
            brightness: 0.4
            contrast: 0.4
            saturation: 0.2
            hue: 0.1
            p: 0.8
          - name: random_gray_scale
            p: 0.2
          - name: random_gaussian_blur_random_radius
            p:
            - 1.0
            - 0.1
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            radius_min: 0.1
            radius_max: 2.0
          - name: random_solarize
            threshold: 128
            p:
            - 0.0
            - 0.2
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
          - name: to_tensor
          - name: normalize
            mean:
            - 0.485
            - 0.456
            - 0.406
            std:
            - 0.229
            - 0.224
            - 0.225
        masked_patch_transform:
          image_size: 224
          patch_size: 16
          mask_probability: 0.5
          mask_ratio_tuple:
          - 0.1
          - 0.5
