# This config is from rel-2.4.2 and it is derived from params_dinov2_large_patch16_img224_cszoov2.yaml
# Differences to params_dinov2_large_patch16_img224_cszoov2.yaml:
# * patch_size: 14 (instead of 16)
# * separate DinoHead and iBOTHead (instead of shared)
# * batch_size: 1024 (instead of 2048)
# * use_projection_bias_in_attention: false (instead of true)
# * use_ffn_bias_in_attention: false (instead of true)
# * use_ffn_bias: false (instead of true)

trainer:
  init:
    seed: 1
    backend:
      backend_type: CSX
      cluster_config:
        num_csx: 1
        # Increase workers for more performance if there are more worker nodes.
        num_workers_per_csx: 2
    model:
      name: dino
      image_model_trunks:
      - image_model:
          name: MultiImageViTModel
          position_embedding_type: learned
          embedding_dropout_rate: 0.0
          hidden_size: 1024
          use_post_embed_layer_norm: false
          use_embed_proj_bias: true
          interpolate_position_embedding: &id001
            antialias: false
            interpolate_offset: 0.1
            local_patch_dims:
            - 7
            - 7
          projection_initializer: &id002
            name: kaiming_uniform
            a: 2.2360679775
          position_embedding_initializer: &id003
            name: truncated_normal
            std: 0.02
            a: -2.0
            b: 2.0
          cls_token_initializer: &id004
            name: normal
            std: 1.0e-06
          default_initializer: &id005
            name: truncated_normal
            std: 0.02
            a: -2.0
            b: 2.0
          num_hidden_layers: 24
          layer_norm_epsilon: 1.0e-06
          layerscale_value: 0.01 # src repo has default to 1.e-5 but we use 0.01
          num_heads: 16
          attention_type: scaled_dot_product
          attention_softmax_fp32: true
          dropout_rate: 0.0
          nonlinearity: gelu
          attention_dropout_rate: 0.0
          use_projection_bias_in_attention: false
          use_ffn_bias_in_attention: false
          filter_size: 4096 # src repo has default to swiglu but checkpoint has 4096 so they must have gelu
          use_ffn_bias: false
          initializer_range: 0.02
          norm_first: true
          image_size: &id006
          - 224
          - 224
          num_channels: 3
          patch_size: &id007
          - 14
          - 14
          use_conv_patchified_embedding: false
          use_encoder_pooler_layer: false
          prepend_cls_token: true
          use_masked_patches: true
        forward_args:
        - - ssl_transform.output('global_view')
          - null
          - null
        stop_grad: true
      - image_model:
          name: MultiImageViTModel
          position_embedding_type: learned
          embedding_dropout_rate: 0.0
          hidden_size: 1024
          use_post_embed_layer_norm: false
          use_embed_proj_bias: true
          interpolate_position_embedding: *id001
          projection_initializer: *id002
          position_embedding_initializer: *id003
          cls_token_initializer: *id004
          default_initializer: *id005
          num_hidden_layers: 24
          layer_norm_epsilon: 1.0e-06
          layerscale_value: 0.01
          num_heads: 16
          attention_type: scaled_dot_product
          attention_softmax_fp32: true
          dropout_rate: 0.0
          nonlinearity: gelu
          attention_dropout_rate: 0.0
          use_projection_bias_in_attention: false
          use_ffn_bias_in_attention: false
          filter_size: 4096
          use_ffn_bias: false
          initializer_range: 0.02
          norm_first: true
          image_size: *id006
          num_channels: 3
          patch_size: *id007
          use_conv_patchified_embedding: false
          use_encoder_pooler_layer: false
          prepend_cls_token: true
          use_masked_patches: true
          stochastic_depth_drop_prob: 0.3
          stochastic_depth_mode: row
          stochastic_depth_drop_prob_schedule: constant
        forward_args:
        - - ssl_transform.output('global_view')
          - null
          - ssl_transform.output('collated_masks')
        - - ssl_transform.output('local_view')
          - null
          - null
        stop_grad: false
      heads:
      - head_model: &id008
          name: DinoHead
          input_size: 1024
          hidden_size: 2048
          output_size: 65536
          bottleneck_size: 256
          norm_last_layer: false
          num_layers: 3
          initializer:
            name: truncated_normal
            std: 0.02
        forward_args:
        - - image_model_trunks.output(0,0,1)
        stop_grad: true
      - head_model: *id008
        forward_args:
        - - image_model_trunks.output(1,0,1)
        - - image_model_trunks.output(1,1,1)
        stop_grad: false
      - head_model: *id008
        forward_args:
        - - image_model_trunks.output(0,0,0)
        stop_grad: true
      - head_model: *id008
        forward_args:
        - - image_model_trunks.output(1,0,0)
        stop_grad: false
      losses:
      - loss:
          name: DinoDistillationLoss
          input_size: 65536
          warmup_teacher_temp: 0.04
          teacher_temp: 0.07
          warmup_teacher_temp_steps: 37500
          total_steps: 125000
          student_temp: 0.1
          center_momentum: 0.9
          teacher_temp_scheduler: linear_cosine
        forward_args:
        - - heads.output(1,0,0)
          - heads.output(0,0,0)
          - heads.output(1,1,0)
        stop_grad: false
      - loss:
          name: iBOTPatchLoss
          input_size: 65536
          warmup_teacher_temp: 0.04
          teacher_temp: 0.07
          warmup_teacher_temp_steps: 37500
          total_steps: 125000
          student_temp: 0.1
          center_momentum: 0.9
          teacher_temp_scheduler: linear_cosine
        forward_args:
        - - heads.output(2,0,0)
          - heads.output(3,0,0)
          - ssl_transform.output('collated_masks')
        stop_grad: false
      copy_init_weights:
      - source: image_model_trunks.model(0)
        target: image_model_trunks.model(1)
      - source: heads.model(0)
        target: heads.model(1)
      - source: heads.model(2)
        target: heads.model(3)
      ema:
      - source: image_model_trunks.model(1)
        target: image_model_trunks.model(0)
        scheduler_name: cosine
        scheduler_params:
          ema_decay_start: 0.992
          ema_decay_end: 1.0
          total_steps: 125000
      - source: heads.model(1)
        target: heads.model(0)
        scheduler_name: cosine
        scheduler_params:
          ema_decay_start: 0.992
          ema_decay_end: 1.0
          total_steps: 125000
      - source: heads.model(3)
        target: heads.model(2)
        scheduler_name: cosine
        scheduler_params:
          ema_decay_start: 0.992
          ema_decay_end: 1.0
          total_steps: 125000
      freeze:
      - image_model_trunks.model.0.*
      - heads.model.0.*
      - heads.model.2.*
    optimizer:
      AdamW:
        betas:
        - 0.9
        - 0.999
        eps: 1.0e-08
        correct_bias: true
    schedulers:
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.004
            total_iters: 12500
        - CosineDecayLR:
            initial_learning_rate: 0.004
            end_learning_rate: 1.0e-06
            total_iters: 112500
    precision:
      enabled: true
      fp16_type: cbfloat16
      loss_scaling_factor: dynamic
      max_gradient_norm: 1.0
      precision_opt_level: 1
      log_loss_scale: true
    loop:
      max_steps: 125000
    logging:
      log_steps: 1
    callbacks:
    - ScopedTrainFlags:
        csx.performance.micro_batch_size: 256
  fit:
    train_dataloader:
      data_processor: DinoImageDataProcessor
      batch_size: 1024
      shuffle: true
      shuffle_seed: 1456354
      num_workers: 2
      prefetch_factor: 2
      dataset:
        name: Imagenet
        root: ./computer_vision/datasets/imagenet/imagenet1k_ilsvrc2012
        split: train
      ssl_transform:
        name: Dinov2Transform
        multi_crop_transform:
          global_num_crops: 2
          local_num_crops: 8
          global_image_size: 224
          local_image_size: 98
          global_crops_scale:
          - 0.32
          - 1
          local_crops_scale:
          - 0.05
          - 0.32
          multicrop_transform_list:
          - name: random_horizontal_flip
            p: 0.5
          - name: color_jitter_with_prob
            brightness: 0.4
            contrast: 0.4
            saturation: 0.2
            hue: 0.1
            p: 0.8
          - name: random_gray_scale
            p: 0.2
          - name: random_gaussian_blur_random_radius
            p:
            - 1.0
            - 0.1
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            radius_min: 0.1
            radius_max: 2.0
          - name: random_solarize
            threshold: 128
            p:
            - 0.0
            - 0.2
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
          - name: to_tensor
          - name: normalize
            mean:
            - 0.485
            - 0.456
            - 0.406
            std:
            - 0.229
            - 0.224
            - 0.225
        masked_patch_transform:
          image_size: 224
          patch_size: 14
          mask_probability: 0.5
          mask_ratio_tuple:
          - 0.1
          - 0.5
    val_dataloader:
      data_processor: DinoImageDataProcessor
      batch_size: 256
      shuffle: true
      shuffle_seed: 1456354
      num_workers: 2
      prefetch_factor: 2
      dataset:
        name: Imagenet
        root: ./computer_vision/datasets/imagenet/imagenet1k_ilsvrc2012
        split: train
      ssl_transform:
        name: Dinov2Transform
        multi_crop_transform:
          global_num_crops: 2
          local_num_crops: 8
          global_image_size: 224
          local_image_size: 98
          global_crops_scale:
          - 0.32
          - 1
          local_crops_scale:
          - 0.05
          - 0.32
          multicrop_transform_list:
          - name: random_horizontal_flip
            p: 0.5
          - name: color_jitter_with_prob
            brightness: 0.4
            contrast: 0.4
            saturation: 0.2
            hue: 0.1
            p: 0.8
          - name: random_gray_scale
            p: 0.2
          - name: random_gaussian_blur_random_radius
            p:
            - 1.0
            - 0.1
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            - 0.5
            radius_min: 0.1
            radius_max: 2.0
          - name: random_solarize
            threshold: 128
            p:
            - 0.0
            - 0.2
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
            - 0.0
          - name: to_tensor
          - name: normalize
            mean:
            - 0.485
            - 0.456
            - 0.406
            std:
            - 0.229
            - 0.224
            - 0.225
        masked_patch_transform:
          image_size: 224
          patch_size: 14
          mask_probability: 0.5
          mask_ratio_tuple:
          - 0.1
          - 0.5