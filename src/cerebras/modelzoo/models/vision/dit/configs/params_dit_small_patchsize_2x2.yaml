# The configs here are set to use image size of `256 x 256`.
# To use with image size `512 x 512`, please make the following changes:
# 1. train_input.image_size: [512, 512]
# 2. model.vae.latent_size: [64, 64]
# 3. train_input.transforms(if any): change `size` params under various transforms to [512, 512] 

train_input: &train_input
  # NOTE: when using this input, model.latent_size should be set to [32, 32]
  data_processor: "DiffusionLatentImageNet1KProcessor"
  data_dir: 
    - "./computer_vision/datasets/imagenet1k_ilsvrc2012_vaelatent_im256"
    - "./computer_vision/datasets/imagenet1k_ilsvrc2012_vaelatent_im256_hflipped"
  image_size: [256, 256]  # [H, W]
  image_channels: 3
  # In distributed training, batch_size specifies the per-GPU batch size;
  # thus, you should divide by the number of GPU used.
  # The effective batch size, which is evenly divided across "num_csx" systems used for the run
  batch_size: 256
  shuffle: True
  num_classes: 1000
  num_workers: 8
  split: "train"
  num_diffusion_steps: 1000

eval_input:
  # This section should only be used for loss
  # NOTE: when using this input, model.latent_size should be set to [32, 32]
  <<: *train_input
  split: val
  shuffle: False

model:
    # diffusion settings
    schedule_name: "linear"

    # Embedding
    embedding_dropout_rate: 0.0
    hidden_size: 384 
    embedding_nonlinearity: "silu"
    position_embedding_type: "fixed"

    # Time embedding
    frequency_embedding_size: 256

    # Label embedding
    label_dropout_rate: 0.1

    # Encoder
    num_hidden_layers: 12
    layer_norm_epsilon: 1.0e-6

    # Encoder Attn
    num_heads: 6
    attention_type: "scaled_dot_product"
    attention_softmax_fp32: True
    dropout_rate: 0.0
    encoder_nonlinearity: "gelu"
    attention_dropout_rate: 0.0
    use_projection_bias_in_attention: True
    use_ffn_bias_in_attention: True

    # Encoder ffn
    filter_size: 1536
    use_ffn_bias: True

    # Task-specific
    initializer_range: 0.02
    norm_first: True
    
    # vision related params
    patch_size: [2, 2]
    use_conv_patchified_embedding: True

    mixed_precision: True
    fp16_type: "bfloat16"

    vae:
      # https://huggingface.co/stabilityai/sd-vae-ft-ema/blob/main/config.json
      down_block_types: [
          "DownEncoderBlock2D",
          "DownEncoderBlock2D",
          "DownEncoderBlock2D",
          "DownEncoderBlock2D"
      ]
      up_block_types: [
          "UpDecoderBlock2D",
          "UpDecoderBlock2D",
          "UpDecoderBlock2D",
          "UpDecoderBlock2D"
      ]
      block_out_channels: [128, 256, 512, 512]
      layers_per_block: 2
      act_fn: "silu"
      latent_size: [32, 32]  # output height and width expected from Encoder
      latent_channels: 4
      norm_num_groups: 32
      sample_size: 256
      scaling_factor: 0.18215

    reverse_process:
      sampler:
        # Can pass all kwargs that are in _init_ of sampler
        name: "ddpm"
        beta_start: 0.0001
        beta_end: 0.02
        num_inference_steps: 250
        # Specify either `num_inference_steps` or `custom_timesteps`
        # custom_timesteps: [999, 749, 654, 480, 375, 125, 0]    
      pipeline:
        # guidance_scale = 1.0 disables classifier-free guidance,
        # Refer to Appendix A 
        guidance_scale: 1.0  
        num_cfg_channels: 3
        # set `custom_labels` to generate samples related 
        # to particular imagenet ids
        # custom_labels: [207, 360, 387, 974, 88, 979, 417, 279]

optimizer:
    optimizer_type: "AdamW"
    learning_rate: 0.0001
    weight_decay: 0.0

runconfig:
    log_steps: 1
    checkpoint_steps: 50000
    max_steps: 400000
    seed: 1
    num_workers_per_csx: 1
    save_initial_checkpoint: True
    precision_opt_level: 1
    # # enable GPU distributed training:
    # enable_distributed: True
    # dist_addr: "localhost:8888"
    # dist_backend: "nccl"
    # init_method: "env://"
    # sync_batchnorm: True