# TODO: Correct param values for optimizer and runconfig
# LLaVA - 13B
# Text model: Vicuna-13B
# Image model: CLIP ViT Large with patch 14

trainer:
  init:
    backend:
      backend_type: CSX
    seed: 1
    model:
      name: "llava"
      image_model:
        name: ViTModel
        # Embedding
        position_embedding_type: learned
        embedding_dropout_rate: 0.0
        hidden_size: 1024
        use_post_embed_layer_norm: true
        use_embed_proj_bias: false
        # Encoder
        num_hidden_layers: 24
        layer_norm_epsilon: 1.0e-05
        # Encoder Attn
        num_heads: 16
        attention_type: scaled_dot_product
        attention_softmax_fp32: true
        dropout_rate: 0.0
        nonlinearity: quick_gelu
        # pooler_nonlinearity: "tanh"
        attention_dropout_rate: 0.0
        use_projection_bias_in_attention: true
        use_ffn_bias_in_attention: true
        # Encoder ffn
        filter_size: 4096
        use_ffn_bias: true
        # Task-specific
        initializer_range: 0.02
        norm_first: true
        # vision related params
        image_size:
        - 336
        - 336
        num_channels: 3
        patch_size:
        - 14
        - 14
        use_conv_patchified_embedding: true
        use_encoder_pooler_layer: false
        prepend_cls_token: true
      text_model:
        name: LlamaModel
        vocab_size: 32000
        hidden_size: 5120
        position_embedding_type: rotary
        rotary_dim: 128 # hidden_size// num_heads
        share_embedding_weights: false
        max_position_embeddings: 2048 ### MSL 2K
        embd_pdrop: 0.0
        # Decoder
        num_hidden_layers: 40
        dropout_rate: 0.0
        layer_norm_epsilon: 1.0e-05
        norm_type: rmsnorm
        # Decoder - Attention
        num_heads: 40
        attention_type: scaled_dot_product
        attention_dropout_rate: 0.0
        use_projection_bias_in_attention: false
        use_ffn_bias_in_attention: false
        # Decoder - ffn
        filter_size: 13824
        nonlinearity: swiglu
        use_ffn_bias: false
        # Task-specific
        use_bias_in_output: false
      projector:
        image_model:
          name: FeedForwardNetwork
          input_unit: 1024 # image_model.hidden_size
          layers_units:
          - 5120 # layers_units: [5120, 5120]  # [text_model.hidden_size]*num_linear_layers
          - 5120
          layers_activation:
          - gelu # ["gelu", null] # use `null` to specify None
          -
          use_bias: true
      # Loss scaling
      loss_scaling: num_tokens
      loss_weight: 1.0
      freeze:
      - ^image_model #freeze: ['^image_model', '^text_model'] # module is frozen if name matches regex
      - ^text_model
      image_feature_select_layer_idx: -2
      image_start_idx: 1 # position in MSL where image tokens start
      image_feature_select_mode: patch
    optimizer:
      AdamW:
        betas:
        - 0.9
        - 0.999
        eps: 1.0e-08
        correct_bias: true
    schedulers:
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.001
            total_iters: 66
        - CosineDecayLR:
            initial_learning_rate: 0.001
            end_learning_rate: 0.0
            total_iters: 2144
    precision:
      enabled: true
      fp16_type: cbfloat16
      loss_scaling_factor: dynamic
      max_gradient_norm: 1.0
      log_loss_scale: true
    loop:
      max_steps: 2180 # 1 epoch of 565128 samples
      eval_frequency: 1000
      eval_steps: 1000
    checkpoint:
      steps: 1000
    logging:
      log_steps: 1
    callbacks:
    - ScopedTrainFlags:
        csx.performance.micro_batch_size: 55
    - ScopedValidateFlags:
        csx.performance.micro_batch_size: 55
    - ComputeNorm: {}
    - KeepNCheckpoints:
        n: 3
  fit:
    train_dataloader:
      data_processor: LlavaHDF5MapDataProcessor
      data_dir: ./llava_phase_1_shuffled_4_features
      img_data_dir: ./LLaVA/LLaVA-Pretrain/images
      image_data_size:
      - 3
      - 336
      - 336
      transforms:
      - name: expand_to_square
        background_color:
        - 122
        - 116
        - 104
      - name: resize
        size: 336
        interpolation: bicubic
      - name: center_crop
        size: 336
      - name: to_tensor
      - name: normalize
        mean:
        - 0.48145466
        - 0.4578275
        - 0.40821073
        std:
        - 0.26862954
        - 0.26130258
        - 0.27577711
      shuffle: true
      shuffle_seed: 274508134
      batch_size: 275
      drop_last: true
      num_workers: 8
      prefetch_factor: 1
      persistent_workers: true
    val_dataloader: &val_dataloader
      data_processor: LlavaHDF5MapDataProcessor
      data_dir: ./llava_phase_1_shuffled_4_features
      img_data_dir: ./LLaVA/LLaVA-Pretrain/images
      image_data_size:
      - 3
      - 336
      - 336
      transforms:
      - name: expand_to_square
        background_color:
        - 122
        - 116
        - 104
      - name: resize
        size: 336
        interpolation: bicubic
      - name: center_crop
        size: 336
      - name: to_tensor
      - name: normalize
        mean:
        - 0.48145466
        - 0.4578275
        - 0.40821073
        std:
        - 0.26862954
        - 0.26130258
        - 0.27577711
      shuffle: false
      batch_size: 275
      drop_last: true
      num_workers: 8
      prefetch_factor: 1
      persistent_workers: true
  validate:
    val_dataloader: *val_dataloader
  validate_all:
    val_dataloaders: *val_dataloader
