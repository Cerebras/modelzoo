# LLaVA - 7B
# Text model: Vicuna-7B
# Image model: CLIP ViT Large with patch 14

trainer:
  init:
    backend:
      backend_type: CSX
    seed: 1
    model:
      name: "llava"
      image_model:
        name: ViTModel
        # Embedding
        position_embedding_type: learned
        embedding_dropout_rate: 0.0
        hidden_size: 1024
        use_post_embed_layer_norm: true
        use_embed_proj_bias: false
        # Encoder
        num_hidden_layers: 24
        layer_norm_epsilon: 1.0e-05
        # Encoder Attn
        num_heads: 16
        attention_type: scaled_dot_product
        attention_softmax_fp32: true
        dropout_rate: 0.0
        nonlinearity: quick_gelu
        # pooler_nonlinearity: "tanh"
        attention_dropout_rate: 0.0
        use_projection_bias_in_attention: true
        use_ffn_bias_in_attention: true
        # Encoder ffn
        filter_size: 4096
        use_ffn_bias: true
        # Task-specific
        initializer_range: 0.02
        norm_first: true
        # vision related params
        image_size:
        - 336
        - 336
        num_channels: 3
        patch_size:
        - 14
        - 14
        use_conv_patchified_embedding: true
        use_encoder_pooler_layer: false
        prepend_cls_token: true
      text_model:
        name: LlamaModel
        vocab_size: 32000
        hidden_size: 4096
        position_embedding_type: rotary
        rotary_dim: 128 # hidden_size// num_heads
        share_embedding_weights: false
        max_position_embeddings: 2048 #### MSL 2K
        embd_pdrop: 0.0
        # Decoder
        num_hidden_layers: 32
        dropout_rate: 0.0
        layer_norm_epsilon: 1.0e-05
        norm_type: rmsnorm
        # Decoder - Attention
        num_heads: 32
        attention_type: scaled_dot_product
        attention_dropout_rate: 0.0
        use_projection_bias_in_attention: false
        use_ffn_bias_in_attention: false
        # Decoder - ffn
        filter_size: 11008
        nonlinearity: swiglu
        use_ffn_bias: false
        # Task-specific
        use_bias_in_output: false
      projector:
        image_model:
          name: FeedForwardNetwork
          input_unit: 1024 # image_model.hidden_size
          layers_units:
          - 4096 #
          - 4096
          layers_activation:
          - gelu   #
          - 
          use_bias: true
      # Loss scaling
      loss_scaling: num_tokens
      loss_weight: 1.0
      freeze:
      - ^image_model
      image_feature_select_layer_idx: -2
      image_start_idx: 35
      image_feature_select_mode: patch
    optimizer:
      AdamW: # in the paper AdaFactor is used, but AdamW should be a good alternative.
        betas:
        - 0.9
        - 0.999
        eps: 1.0e-08
        # weight_decay: 0.001
        correct_bias: true
    schedulers:
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 2.0e-05
            total_iters: 155
        - CosineDecayLR:
            initial_learning_rate: 2.0e-05
            end_learning_rate: 0.0
            total_iters: 5042
    precision:
      enabled: true
      fp16_type: cbfloat16
      loss_scaling_factor: dynamic
      # weight_decay: 0.001
      max_gradient_norm: 1.0
      log_loss_scale: true
    loop:
      max_steps: 5197
      eval_frequency: 1000
      eval_steps: 1000
    checkpoint:
      steps: 1000
    logging:
      log_steps: 1
    callbacks:
    - ScopedTrainFlags:
        csx.performance.micro_batch_size: 72
    - ScopedValidateFlags:
        csx.performance.micro_batch_size: 72
    - ComputeNorm: {}
    - LoadCheckpointStates:
        load_checkpoint_states: model
    - KeepNCheckpoints:
        n: 3
  fit:
    train_dataloader:
      data_processor: LlavaHDF5MapDataProcessor
      data_dir: ./llava_phase_2_shuffled_4_features
      img_data_dir: ./llava/LLaVA-Instruct-150K/llava_v1_5_images
      image_data_size:
      - 3
      - 336
      - 336
      transforms:
      - name: expand_to_square
        background_color:
        - 122
        - 116
        - 104
      - name: resize
        size: 336
        interpolation: bicubic
      - name: center_crop
        size: 336
      - name: to_tensor
      - name: normalize
        mean:
        - 0.48145466
        - 0.4578275
        - 0.40821073
        std:
        - 0.26862954
        - 0.26130258
        - 0.27577711
      shuffle: true
      shuffle_seed: 274508134
      # The effective batch size, which is evenly divided across "num_csx" systems used for the run
      batch_size: 216
      drop_last: true
      num_workers: 8
      prefetch_factor: 1
      persistent_workers: true
    val_dataloader: &val_dataloader
      data_processor: LlavaHDF5MapDataProcessor
      data_dir: ./llava_phase_2_shuffled_4_features
      img_data_dir: ./llava/LLaVA-Instruct-150K/llava_v1_5_images
      image_data_size:
      - 3
      - 336
      - 336
      transforms:
      - name: expand_to_square
        background_color: # [int(255 * 0.48145466), int(255 * 0.4578275], int(255 * 0.40821073)]
        - 122
        - 116
        - 104
      - name: resize
        size: 336
        interpolation: bicubic
      - name: center_crop
        size: 336
      - name: to_tensor
      - name: normalize
        mean:
        - 0.48145466
        - 0.4578275
        - 0.40821073
        std:
        - 0.26862954
        - 0.26130258
        - 0.27577711
      shuffle: true
      shuffle_seed: 274508134
      batch_size: 216
      drop_last: true
      num_workers: 8
      prefetch_factor: 1
      persistent_workers: true
  validate:
    val_dataloader: *val_dataloader
  validate_all:
    val_dataloaders: *val_dataloader
