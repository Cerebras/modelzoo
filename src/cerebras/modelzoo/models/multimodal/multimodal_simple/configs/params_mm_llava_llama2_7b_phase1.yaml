trainer:
  init:
    backend:
      backend_type: CSX
    seed: 1
    model:
      name: "multimodal_simple"
      image_model_list:
        image_feature_select_mode: patch
        image_models:
        - image_encoder:
            name: ViTModel
            position_embedding_type: learned
            embedding_dropout_rate: 0.0
            hidden_size: 1024
            use_post_embed_layer_norm: true
            use_embed_proj_bias: false
            num_hidden_layers: 24
            layer_norm_epsilon: 1.0e-05
            num_heads: 16
            attention_type: scaled_dot_product
            attention_softmax_fp32: true
            dropout_rate: 0.0
            nonlinearity: quick_gelu
            attention_dropout_rate: 0.0
            use_projection_bias_in_attention: true
            use_ffn_bias_in_attention: true
            filter_size: 4096
            use_ffn_bias: true
            initializer_range: 0.02
            norm_first: true
            image_size:
            - 336
            - 336
            num_channels: 3
            patch_size:
            - 14
            - 14
            use_conv_patchified_embedding: true
            use_encoder_pooler_layer: false
            prepend_cls_token: true
            image_layer_idx: -2
        global_image_projection:
          name: FeedForwardNetwork
          input_unit: 1024
          layers_units:
          - 4096
          - 4096
          layers_activation:
          - gelu
          -
          use_bias: true
      text_model:
        name: LlamaModel
        vocab_size: 32000
        hidden_size: 4096
        position_embedding_type: rotary
        rotary_dim: 128
        share_embedding_weights: false
        embedding_layer_norm: false
        max_position_embeddings: 2048
        pos_scaling_factor: 1.0
        embd_pdrop: 0.0
        num_hidden_layers: 32
        dropout_rate: 0.0
        layer_norm_epsilon: 1.0e-05
        norm_type: rmsnorm
        num_heads: 32
        attention_type: scaled_dot_product
        attention_dropout_rate: 0.0
        use_projection_bias_in_attention: false
        use_ffn_bias_in_attention: false
        filter_size: 11008
        nonlinearity: swiglu
        use_ffn_bias: false
        use_bias_in_output: false
        extra_ffn_params:
          static_dual_expert: false
      freeze:
      - ^image_model.image_model_list
      - ^text_model
      loss_scaling: num_tokens
      loss_weight: 1.0
    optimizer:
      AdamW:
        betas:
        - 0.9
        - 0.999
        eps: 1.0e-08
        weight_decay: 0.01
        correct_bias: true
    schedulers:
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.001
            total_iters: 100
        - CosineDecayLR:
            initial_learning_rate: 0.001
            end_learning_rate: 5.0e-05
            total_iters: 2400
    precision:
      enabled: true
      fp16_type: cbfloat16
      loss_scaling_factor: dynamic
      max_gradient_norm: 1.0
      log_loss_scale: true
    loop:
      max_steps: 2500
      eval_steps: 1000
    checkpoint:
      steps: 200
      disable_strict_checkpoint_loading: true
      save_initial_checkpoint: false
    logging:
      log_steps: 10
    callbacks:
    - ScopedTrainFlags:
        csx.performance.micro_batch_size: 24
    - ScopedValidateFlags:
        csx.performance.micro_batch_size: 24
    - ComputeNorm: {}
    - LoadCheckpointStates:
        load_checkpoint_states: model
    - KeepNCheckpoints:
        n: 1
  fit:
    train_dataloader:
      data_processor: MultimodalSimpleHDF5MapDataProcessor
      data_dir: ./cmm_format_llava_phase_1
      img_data_dir: ./LLaVA/LLaVA-Pretrain/images
      image_data_size:
      - 3
      - 336
      - 336
      transforms:
      - name: expand_to_square
        background_color: # [int(255 * 0.48145466), int(255 * 0.4578275], int(255 * 0.40821073)]
        - 122
        - 116
        - 104
      - name: resize
        size: 336
        interpolation: bicubic
      - name: center_crop
        size: 336
      - name: to_tensor
      - name: normalize
        mean:
        - 0.48145466
        - 0.4578275
        - 0.40821073
        std:
        - 0.26862954
        - 0.26130258
        - 0.27577711
      shuffle: true
      shuffle_seed: 274508134
      batch_size: 220
      drop_last: true
      num_workers: 8
      prefetch_factor: 1
      persistent_workers: true
    val_dataloader: &val_dataloader
      data_processor: MultimodalSimpleHDF5MapDataProcessor
      data_dir: ./cmm_format_llava_phase_1
      img_data_dir: ./LLaVA/LLaVA-Pretrain/images
      image_data_size:
      - 3
      - 336
      - 336
      transforms:
      - name: expand_to_square
        background_color: # [int(255 * 0.48145466), int(255 * 0.4578275], int(255 * 0.40821073)]
        - 122
        - 116
        - 104
      - name: resize
        size: 336
        interpolation: bicubic
      - name: center_crop
        size: 336
      - name: to_tensor
      - name: normalize
        mean:
        - 0.48145466
        - 0.4578275
        - 0.40821073
        std:
        - 0.26862954
        - 0.26130258
        - 0.27577711
      shuffle: false
      shuffle_seed: 274508134
      batch_size: 220
      drop_last: true
      num_workers: 8
      prefetch_factor: 1
      persistent_workers: true
  validate:
    val_dataloader: *val_dataloader
  validate_all:
    val_dataloaders: *val_dataloader
