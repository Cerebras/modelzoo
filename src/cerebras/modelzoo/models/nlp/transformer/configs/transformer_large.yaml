# Transformer Large - according to Table 3.
# Based on transformer_big(), but using transformer_base_v1() instead of transformer_base()
# https://github.com/tensorflow/tensor2tensor/blob/5623deb79cfcd28f8f8c5463b58b5bd76a81fd0d/tensor2tensor/models/transformer.py#L1981

trainer:
  init:
    backend:
      backend_type: CSX
    model_dir: ./model_dir
    seed: 1
    model:
      name: "transformer"
      src_vocab_size: 36550
      tgt_vocab_size: 36550
      ## Encoder
      encoder_num_hidden_layers: 6
      dropout_rate: 0.1
      relu_dropout_rate: 0.0
      use_pre_encoder_decoder_dropout: true
      use_dropout_outside_residual_path: false
      # Encoder -- Attention
      d_kv: 64 # Size of the key, query, value projections per attention head.
      num_heads: 16 # d_kv * num_heads = hidden size(i.e. d_model)
      use_projection_bias_in_attention: false
      # Position Embeddings
      position_embedding_type: fixed
      src_max_position_embeddings: 256
      tgt_max_position_embeddings: 256
      # Shared Weighed Embeddings
      share_embedding_weights: true
      share_encoder_decoder_embedding: true
      norm_type: layernorm # Disable T5 style layer norm (RMSNorm)
      use_pre_encoder_decoder_layer_norm: false
      # Encoder -- ffn
      d_ff: 4096 # Size of the intermediate feed forward layer in t5 blocks.
      d_model: 1024 # Size of the encoder layers and the pooler layer.
      encoder_nonlinearity: relu # {"gelu", "relu", "geglu"}
      decoder_nonlinearity: relu # {"gelu", "relu", "geglu"}
      layer_norm_epsilon: 1.0e-05
      use_ffn_bias: true
      ## Decoder
      decoder_num_hidden_layers: 6
      # Loss scaling weight, 1/{average_number_valid_tokens}
      lm_loss_weight: 0.033
      use_transformer_initialization: true
      # Cerebras configs.
    optimizer:
      Adam:
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
    schedulers:
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.0004941058844013092 # (d_model(=1024)**-0.5) * (warm=4000**-0.5)
            total_iters: 4000
        - LinearLR:
            initial_learning_rate: 0.0004940441327439536
            end_learning_rate: 0.00022790798791340433
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 0.00022790798791340433
            end_learning_rate: 0.0001704801898581574
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 0.0001704801898581574
            end_learning_rate: 0.00014204398715647246
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 0.00014204398715647246
            end_learning_rate: 0.00012430484223196318
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 0.00012430484223196318
            end_learning_rate: 0.00011189223181306404
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 0.00011189223181306404
            end_learning_rate: 0.00010258259796518404
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 0.00010258259796518404
            end_learning_rate: 9.526686369390594e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 9.526686369390594e-05
            end_learning_rate: 8.932181488577381e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 8.932181488577382e-05
            end_learning_rate: 8.436676240964464e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 8.436676240964464e-05
            end_learning_rate: 8.015429509164194e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 8.015429509164194e-05
            end_learning_rate: 7.651572483578392e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 7.651572483578392e-05
            end_learning_rate: 7.33315566062018e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 7.33315566062018e-05
            end_learning_rate: 7.051445835878773e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 7.051445835878773e-05
            end_learning_rate: 6.799892337558537e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 6.799892337558537e-05
            end_learning_rate: 6.573472361805505e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 6.573472361805505e-05
            end_learning_rate: 6.368261354529861e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 6.368261354529861e-05
            end_learning_rate: 6.181142378334902e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 6.181142378334902e-05
            end_learning_rate: 6.009604272191723e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 6.009604272191723e-05
            end_learning_rate: 5.8515982277552135e-05
            total_iters: 14800
        - LinearLR:
            initial_learning_rate: 5.8515982277552135e-05
            end_learning_rate: 5.705433798297074e-05
            total_iters: 14800
    precision:
      # Cerebras configs.
      enabled: true
      loss_scaling_factor: dynamic
    loop:
      max_steps: 300000
      eval_frequency: 10000
      eval_steps: 3001 # comment out this line for GPU eval
    checkpoint:
      steps: 10000
    logging:
      log_steps: 100
  fit:
    train_dataloader:
      data_processor: TransformerDynamicDataProcessor
      src_vocab_file: ./transformer/wmt16_en_de/vocab.bpe.32000.en
      src_data_dir: ./transformer/wmt16_en_de/pytorch/train.tok.clean.bpe.32000.en
      src_max_sequence_length: 256
      tgt_max_sequence_length: 256
      # The effective batch size, which is evenly divided across "num_csx" systems used for the run
      batch_size: 1024
      shuffle: true
      shuffle_seed: 1
      num_workers: 8
      prefetch_factor: 10
      persistent_workers: true
    val_dataloader: &val_dataloader
      data_processor: TransformerDynamicDataProcessor
      src_vocab_file: ./transformer/wmt16_en_de/vocab.bpe.32000.en
      src_data_dir: ./transformer/wmt16_en_de/pytorch/newstest2014.tok.clean.bpe.32000.en
      src_max_sequence_length: 256
      tgt_max_sequence_length: 256
      shuffle: false
      shuffle_seed: 1
      batch_size: 4
      num_workers: 4
      prefetch_factor: 10
      persistent_workers: true
  validate:
    val_dataloader: *val_dataloader
  validate_all:
    val_dataloaders: *val_dataloader
