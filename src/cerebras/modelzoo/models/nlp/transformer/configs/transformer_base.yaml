# Transformer Base - according to Table 3.
# Based on transformer_base_v1() :
# https://github.com/tensorflow/tensor2tensor/blob/5623deb79cfcd28f8f8c5463b58b5bd76a81fd0d/tensor2tensor/models/transformer.py#L1771

trainer:
  init:
    backend:
      backend_type: CSX
    model_dir: ./model_dir
    seed: 1
    model:
      name: "transformer"
      src_vocab_size: 36550
      tgt_vocab_size: 36550
      ## Encoder
      encoder_num_hidden_layers: 6
      dropout_rate: 0.1
      relu_dropout_rate: 0.0
      use_pre_encoder_decoder_dropout: true
      use_dropout_outside_residual_path: false
      # Encoder -- Attention
      d_kv: 64 # Size of the key, query, value projections per attention head.
      num_heads: 8 # d_kv * num_heads = hidden size(i.e. d_model)
      use_projection_bias_in_attention: false
      # Position Embeddings
      position_embedding_type: fixed
      src_max_position_embeddings: 256
      tgt_max_position_embeddings: 256
      # Shared Weighed Embeddings
      share_embedding_weights: true
      share_encoder_decoder_embedding: true
      norm_type: layernorm   # Disable T5 style layer norm (RMSNorm)
      use_pre_encoder_decoder_layer_norm: false
      # Encoder -- ffn
      d_ff: 2048 # Size of the intermediate feed forward layer in t5 blocks.
      d_model: 512 # Size of the encoder layers and the pooler layer.
      encoder_nonlinearity: relu   # {"gelu", "relu", "geglu"}
      decoder_nonlinearity: relu   # {"gelu", "relu", "geglu"}
      layer_norm_epsilon: 1.0e-05
      use_ffn_bias: true
      ## Decoder
      decoder_num_hidden_layers: 6
      # Loss scaling weight, 1/{average_number_valid_tokens}
      lm_loss_weight: 0.033
      use_transformer_initialization: true
      # Loss scaling config: `precomputed_num_masked` (for vts only)
      #                      `batch_size` is scaling loss by batch size
      mlm_loss_scaling: precomputed_num_masked
      # Cerebras configs.
    optimizer:
      Adam:
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
    schedulers:
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 0.0
            end_learning_rate: 0.000698683912937353 # (d_model(=512)**-0.5) * (warm=4000**-0.5)
            total_iters: 4000
        - LinearLR:
            initial_learning_rate: 0.000698683912937353
            end_learning_rate: 0.0004710847104863831
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.0004710847104863831
            end_learning_rate: 0.00037894798246424215
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00037894798246424215
            end_learning_rate: 0.0003257949189616652
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.0003257949189616652
            end_learning_rate: 0.00029014271289331547
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00029014271289331547
            end_learning_rate: 0.000264105988466404
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.000264105988466404
            end_learning_rate: 0.00024401778319601815
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00024401778319601815
            end_learning_rate: 0.00022791101850398209
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00022791101850398209
            end_learning_rate: 0.00021462335024901533
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00021462335024901533
            end_learning_rate: 0.00020341801856705902
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00020341801856705902
            end_learning_rate: 0.00019380240931989222
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00019380240931989222
            end_learning_rate: 0.00018543300176721207
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00018543300176721207
            end_learning_rate: 0.00017806195541597027
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00017806195541597027
            end_learning_rate: 0.00017150536478269344
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00017150536478269344
            end_learning_rate: 0.00016562350566866137
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00016562350566866137
            end_learning_rate: 0.0001603080628584663
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.0001603080628584663
            end_learning_rate: 0.00015547359888418554
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00015547359888418554
            end_learning_rate: 0.00015105169410466443
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00015105169410466443
            end_learning_rate: 0.00014698682270697125
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00014698682270697125
            end_learning_rate: 0.00014323338788643664
            total_iters: 4800
        - LinearLR:
            initial_learning_rate: 0.00014323338788643664
            end_learning_rate: 0.00013975354982773464
            total_iters: 4800
    precision:
      # Cerebras configs.
      enabled: true
      loss_scaling_factor: dynamic
    loop:
      max_steps: 100000
      eval_frequency: 10000
      eval_steps: 3001 # comment out this line for GPU eval
    checkpoint:
      steps: 10000
    logging:
      log_steps: 100
  fit:
    train_dataloader:
      data_processor: TransformerDynamicDataProcessor
      src_vocab_file: ./transformer/wmt16_en_de/vocab.bpe.32000.en
      src_data_dir: ./transformer/wmt16_en_de/pytorch/train.tok.clean.bpe.32000.en
      src_max_sequence_length: 256
      tgt_max_sequence_length: 256
      # The effective batch size, which is evenly divided across "num_csx" systems used for the run
      batch_size: 2048 # for GPU(16GB) set batch_size 16
      # 16 * 256 = 4096 with grad_accum_steps: 256
      shuffle: true
      shuffle_seed: 1
      num_workers: 8
      prefetch_factor: 10
      persistent_workers: true
    val_dataloader: &val_dataloader
      data_processor: TransformerDynamicDataProcessor
      src_vocab_file: ./transformer/wmt16_en_de/vocab.bpe.32000.en
      src_data_dir: ./transformer/wmt16_en_de/pytorch/newstest2014.tok.clean.bpe.32000.en
      src_max_sequence_length: 256
      tgt_max_sequence_length: 256
      shuffle: false
      shuffle_seed: 1
      batch_size: 4
      num_workers: 4
      prefetch_factor: 10
      persistent_workers: true
  validate:
    val_dataloader: *val_dataloader
  validate_all:
    val_dataloaders: *val_dataloader
