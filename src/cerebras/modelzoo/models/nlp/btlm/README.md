# BTLM

Released in July 2023, BTLM quickly became the most downloaded model of its size on [Hugging Face](https://huggingface.co/cerebras/btlm-3b-8k-base), amassing over 1 million downloads in three weeks. [BTLM](https://www.cerebras.net/machine-learning/btlm-3b-8k-7b-performance-in-a-3-billion-parameter-model/) is a very similar architecture to [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) with the exception of using [Maximal Update Parameterization (&mu;P)](https://arxiv.org/abs/2203.03466) and adding [SwiGLU activation](https://arxiv.org/abs/2002.05202) and [ALiBi](https://arxiv.org/abs/2108.12409) for [performance improvements](https://arxiv.org/abs/2210.15424). More details can be found in in the [BTLM paper](https://arxiv.org/abs//2309.11568).

For more information on using BTLM, visit its [model page](https://training-docs.cerebras.ai/rel-2.5.0/model-zoo/models/nlp/btlm) in our documentation.
