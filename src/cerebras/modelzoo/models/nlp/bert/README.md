# Bidirectional Transformers for Language Understanding (BERT)

Bidirectional Transformers for Language Understanding ([BERT](https://arxiv.org/abs/1810.04805)) is an encoder-only transformer-based model designed for natural language understanding. This directory contains implementations of the BERT model. It uses a stack of transformer blocks with multi-head attention followed by a multi-layer perceptron feed-forward network. We support removing next-sentence-prediction (NSP) loss from BERT training processing with only masked-language-modeling (MLM) loss.

For more information on using our BERT implementation, visit its [model page](https://training-docs.cerebras.ai/rel-2.5.0/model-zoo/models/nlp/bert) in our documentation.
