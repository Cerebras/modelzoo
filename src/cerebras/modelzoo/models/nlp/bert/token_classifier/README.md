# PyTorch BERT fine-tuning token classification model

- [PyTorch BERT fine-tuning token classification model](#pytorch-bert-fine-tuning-token-classification-model)
  - [Model overview](#model-overview)
  - [Sequence of steps to perform](#sequence-of-steps-to-perform)
  - [Code structure](#code-structure)
  - [Dataset generation](#dataset-generation)
    - [Data download](#data-download)
    - [Data preparation](#data-preparation)
    - [Input function description](#input-function-description)
      - [Features labels dictionary](#features-labels-dictionary)
  - [Run fine-tuning](#run-fine-tuning)
  - [To compile/validate, run train and eval on Cerebras System](#to-compilevalidate-run-train-and-eval-on-cerebras-system)
  - [To run train and eval on GPU/CPU](#to-run-train-and-eval-on-gpucpu)
  - [YAML config file description](#yaml-config-file-description)
  - [References](#references)

## Model overview

This directory contains implementation of the BERT fine-tuning model for token classification task, also referred to as Named Entity Recognition (NER). We provide the BC5-chem dataset as an example from the [BLURB dataset](https://microsoft.github.io/BLURB/tasks.html). The named entity recognition task is used to tag named entities in the input text, this is labeled as `B` conventionally. Some times, the entity is broken into two or more words, then the following entities are labelled as `I`. The words which are not named entities are labelled as `O`. If an entity is broken into multiple tokens, then the subsequent tokens are labelled as `X`.

This is a fine-tuning task, meaning, the BERT model [[1](#references)] is initialized with the weights generated by pre-training the model on a masked language modeling task. The model architecture is adjusted by replacing the masked language modeling (MLM) and next sentence prediction (NSP) output heads used in pre-training with a token classification head. The new output head created by feeding the token representations into an output layer for token-level tasks. This new output head layer is initialized with random weights.

## Sequence of steps to perform

The following block diagram shows a high-level view of the sequence of steps you will perform in this example.

<p align = "center">
<img src = ./images/steps-pt-token-classifier.png>
</p>
<p align = "center">
Fig.1 - Flow Chart of steps to fine-tune token classifier model
</p>

## Code structure

- `configs/`: YAML configuration files.
- `data/nlp/bert/`: Input pipeline implementation for the above mentioned datasets.
- `model.py`: Model implementation leveraging [BertForTokenClassification](../bert_model.py) class.
- `run.py`: Training script. Performs training and validation.
- `utils.py`: Miscellaneous helper functions.

## Dataset generation

### Data download

Download and pre-process the dataset from [BLURB dashboard](https://microsoft.github.io/BLURB/submit.html):

```bash
wget https://microsoft.github.io/BLURB/sample_code/data_generation.tar.gz

tar -xf data_generation.tar.gz
```

Refer to `README.md` from the `data_generation` (downloaded and extracted in [Data Download](#data-download) step) folder to download and pre-process the data for NER.

### Data preparation

Generate CSV files from the raw data using the script `write_csv_ner.py`

```bash

python write_csv_ner.py \
    --data_dir=/path/to/BC5CDR-chem \
    --vocab_file=/path/to/uncased_pubmed_abstracts_and_fulltext_vocab.txt \
    --output_dir=/path/to/bc5dr-chem-csv \
    --do_lower_case
```

Run `python write_csv_ner.py --help` for more information about the arguments.

### Input function description

The input to the model takes in one sentence for which the named entities need to be tagged. Every sentence is preceded by the `[CLS]` special token and is followed by the `[SEP]` special token. The BERT model pre-training uses a special segment embedding layer at the input to signify the two sentences used in the NSP task.

The labels for this model is a sequence of the same length as the input ids and has one of the labels as indices from `"[PAD]", "B", "I", "O", "X", "[CLS]", "[SEP]"`.

If you want to use your own input function with this example code, then this section describes the input data format expected by `BertForTokenClassificationModel` class defined in [model.py](./model.py). When you create your own custom BERT Token Classifier input function, you must ensure that your input function produces a features dictionary and a label tensor as described in this section.

#### Features labels dictionary

The input features and labels are passed to the model in a single dictionary. The features dictionary has the following key/values:

- `input_ids`: Input token IDs, padded with `0` to `max_sequence_length`. The tokens in the dataset are mapped to these IDs using the vocabulary file. These values should be between `0` and `vocab_size - 1`, inclusive. The first token should be the special `[CLS]` token. The end of each sentence should be marked by the special `[SEP]` token. So, a sentence pair should be separated by additional `[SEP]` token.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `attention_mask`: Mask for padded positions. Has values `0` on the padded positions and `1` elsewhere.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `token_type_ids`: Segment IDs. A tensor of the same size as the `input_ids` designating to which segment each token belongs. It is all zeros as there is only one sentence in the input sequence.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

- `labels`: The labels for this model is a sequence of the same length as the `input_ids` and has one of the labels from indices corresponding to `"[PAD]", "B", "I", "O", "X", "[CLS]", "[SEP]"`.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `torch.int32`

## Run fine-tuning

**IMPORTANT**: See the following notes before proceeding further.

**Parameter settings in YAML config file**: The config YAML files are located in the [configs](configs/) directory. Before starting a pre-training run, make sure that in the YAML config file you are using:

- The `train_input.data_dir` parameter points to the correct dataset, and
- The `train_input.max_sequence_length` parameter corresponds to the sequence length of the dataset.
- The `train_input.batch_size` parameter will set the batch size for the training.
- The `train_input.label_vocab_file` generated during the [Data Preparation](#data-preparation) step.

Same applies for the `eval_input`.

**YAML config file differences**:

Please check [YAML config section](#yaml-config-file-description) for details on each config supported out of the box for this model.

## To compile/validate, run train and eval on Cerebras System

Please follow the instructions on our [quickstart in the Developer Docs](https://docs.cerebras.net/en/latest/wsc/getting-started/cs-appliance.html).

> **Note**: To specify a BERT pretrained checkpoint use: `--checkpoint_path` is the path to the saved checkpoint from BERT pre-training,`--load_checkpoint_states="model"` setting as well as the `--disable_strict_checkpoint_loading` flag are needed for loading the pre-trained BERT model for fine-tuning.

## To run train and eval on GPU/CPU

If running on a cpu or gpu, activate the environment from [Python GPU Environment setup](../../../../../../../PYTHON-SETUP.md), and simply run:

```
python run.py CPU --mode train --params /path/to/yaml --model_dir /path/to/model_dir
```
or
```
python run.py GPU --mode train --params /path/to/yaml --model_dir /path/to/model_dir
```

> **Note**: Change the command to `--mode eval` for evaluation.


The description of the command-line arguments is the same as in the above section.

## YAML config file description

- `bert_base_*.yaml` have the standard bert-base config with `hidden_size=768, num_hidden_layers=12, num_heads=12` as a backbone.

## References

[1] [BERT paper](https://arxiv.org/abs/1810.04805)
