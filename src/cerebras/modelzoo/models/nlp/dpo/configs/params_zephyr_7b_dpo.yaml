# DPO + Mistral model, 7B parameters, max_seq_length 1K
# Based on: https://huggingface.co/alignment-handbook/zephyr-7b-dpo-full

trainer:
  init:
    backend:
      backend_type: CSX
    seed: 1
    model:
      name: "dpo"
      dpo:
        beta: 0.01
        reference_free: false
      compute_eval_metrics: true
      policy_model:
        name: mistral
        # Embedding
        vocab_size: 32000
        hidden_size: 4096
        position_embedding_type: rotary
        rotary_dim: 128
        share_embedding_weights: false
        max_position_embeddings: 32768
        embedding_dropout_rate: 0.0
        # Decoder
        num_hidden_layers: 32
        dropout_rate: 0.0
        layer_norm_epsilon: 1.0e-05
        norm_type: rmsnorm
        # Decoder - Attention
        num_heads: 32
        attention_type: scaled_dot_product
        attention_module: multiquery_attention
        extra_attention_params:
          num_kv_groups: 8
        attention_dropout_rate: 0.0
        use_projection_bias_in_attention: false
        use_ffn_bias_in_attention: false
        attention_sliding_window_length: 4096
        # Decoder - ffn
        filter_size: 14336
        nonlinearity: swiglu
        use_ffn_bias: false
        # Task-specific
        use_bias_in_output: false
    optimizer:
      AdamW:
        betas:
        - 0.9
        - 0.999
        eps: 1.0e-08
        correct_bias: true
        weight_decay: 0.0
    schedulers:
    - SequentialLR:
        schedulers:
        - LinearLR:
            initial_learning_rate: 1.0e-08
            end_learning_rate: 5.0e-07
            total_iters: 48 # 10% warmup steps
        - CosineDecayLR:
            initial_learning_rate: 5.0e-07
            end_learning_rate: 4.0e-10
            total_iters: 437
    precision:
      enabled: true
      # Cerebras parameters
      fp16_type: cbfloat16
      precision_opt_level: 1
      loss_scaling_factor: dynamic
      max_gradient_norm: 1.0
      log_loss_scale: true
    loop:
      max_steps: 485 # 3*149
      eval_frequency: 128
      eval_steps: 15
    checkpoint:
      steps: 100
      save_initial_checkpoint: false
    logging:
      log_steps: 10
    callbacks:
    - ScopedTrainFlags:
        csx.performance.micro_batch_size: 32
    - ScopedValidateFlags:
        csx.performance.micro_batch_size: 32
    - ComputeNorm: {}
    - LoadCheckpointStates:
        load_checkpoint_states: model
  fit:
    train_dataloader:
      data_processor: DpoHDF5DataProcessor
      data_dir: ./ultrafeedback_binarized/train
      batch_size: 128
      shuffle: true
      shuffle_seed: 1337
      num_workers: 8
      prefetch_factor: 10
      persistent_workers: true
    val_dataloader: &val_dataloader
      data_processor: DpoHDF5DataProcessor
      data_dir: ./ultrafeedback_binarized/test
      batch_size: 128
      shuffle: false
      num_workers: 8
  validate:
    val_dataloader: *val_dataloader
  validate_all:
    val_dataloaders: *val_dataloader
