trainer:
  fit:
    train_dataloader:
      batch_size: 192
      data_dir: ./language/datasets/SlimPajama/train_shuffled_msl2048
      data_processor: GptHDF5MapDataProcessor
      num_workers: 0
      persistent_workers: true
      prefetch_factor: 10
      # repeat: true
      shuffle: false
      shuffle_seed: 1
      use_worker_cache: false
    val_dataloader: &val_dataloader
      batch_size: 95
      data_dir: ./language/datasets/SlimPajama/val_msl2048
      data_processor: GptHDF5MapDataProcessor
      persistent_workers: true
      prefetch_factor: 10
      shuffle: false
      shuffle_seed: 1
      use_worker_cache: false
  init: &init
    checkpoint:
      save_initial_checkpoint: true
      steps: 557
    logging:
      log_level: INFO
      log_steps: 1
    loop:
      eval_steps: 2741
      max_steps: 5563
      num_steps: 5563
    model: &base_model
      name: mixtral
      attention_dropout_rate: 0.0
      attention_softmax_fp32: true
      attention_type: scaled_dot_product
      dropout_rate: 0.0
      embedding_dropout_rate: 0.0
      embedding_initializer:
        a: -0.1734
        b: 0.1734
        mean: 0.0
        name: truncated_normal
        std: 0.0867
      embeddings_scale: 9.1706
      filter_size: 2048
      hidden_size: 768
      initializer:
        a: -0.10011253667748111
        b: 0.10011253667748111
        mean: 0.0
        name: truncated_normal
        std: 0.05005626833874056
      layer_norm_epsilon: 1.0e-05
      max_position_embeddings: 2048
      moe:
        num_shared_experts: 1
        load_balancing_loss_coef: 0.01
        null_expert_bias: 1.1
        num_experts: 4
        router_fp32: true
        routing_algorithm: learned
        top_k: 1
      nonlinearity: swiglu
      num_heads: 12
      num_hidden_layers: 10
      output_layer_initializer:
        a: -0.02238584374107887
        b: 0.02238584374107887
        mean: 0.0
        name: truncated_normal
        std: 0.011192921870539435
      output_logits_scale: 0.3651
      position_embedding_type: alibi
      scale_qk_dot_by_d: true
      share_embedding_weights: true
      use_bias_in_output: false
      use_ffn_bias: true
      use_ffn_bias_in_attention: true
      use_projection_bias_in_attention: true
      vocab_size: 50257
    optimizer: &base_optimizer
      AdamW:
        adjust_learning_rate:
          decoder_kernel: 0.3333333333333333
        betas:
        - 0.9
        - 0.95
        correct_bias: true
        eps: 1.0e-08
        weight_decay: 0.1
    precision: &base_precision
      enabled: true
      fp16_type: bfloat16
      log_loss_scale: true
      loss_scaling_factor: dynamic
      max_gradient_norm: 1.0
      precision_opt_level: 0
    schedulers: &base_schedulers
    - ScalePerParamLR:
        scheduler:
          SequentialLR:
            schedulers:
            - LinearLR:
                end_learning_rate: 0.015625
                initial_learning_rate: 0.0
                total_iters: 953
            - LinearLR:
                end_learning_rate: 0.0
                initial_learning_rate: 0.015625
                total_iters: 4610
    seed: 1
  validate:
    val_dataloader: *val_dataloader
  validate_all:
    val_dataloaders: *val_dataloader