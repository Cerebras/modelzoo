trainer:
  fit:
    train_dataloader:
      batch_size: 1
      data_processor: MultimodalRawDatasetProcessor
      drop_last: true
      image_data_size:
      - 3
      - 336
      - 336
      img_data_dir: /cb/cold/multimodal_datasets/LLaVA/LLaVA-Pretrain/images
      num_workers: 0
      persistent_workers: true
      prefetch_factor: 10
      preprocessing:
        dataset:
          is_multimodal: true
          max_num_img: 1
          num_patches: 576
          use_vsl: false
        processing:
          ftfy_normalizer: NFC
          huggingface_tokenizer: NousResearch/Llama-2-7b-hf
          max_seq_length: 1024
          read_hook: cerebras.modelzoo.data_preparation.data_preprocessing.hooks:pretraining_image_captions_hook
          read_hook_kwargs:
            caption_key: caption
            image_key: image
          seed: 0
          sep_token: null
          shuffle: false
          use_ftfy: true
          wikitext_detokenize: false
        setup:
          data:
            source: ./language/datasets/test_datapreprocessing/toy_datasets/multimodal_pretraining_data/
            type: local
          image_dir: /cb/cold/multimodal_datasets/LLaVA/LLaVA-Pretrain/images
          mode: pretraining
      seed: 0
      shuffle: false
      shuffle_seed: 0
      transforms:
      - background_color:
        - 122
        - 116
        - 104
        name: expand_to_square
      - interpolation: bicubic
        name: resize
        size: 336
      - name: center_crop
        size: 336
      - name: to_tensor
      - mean:
        - 0.48145466
        - 0.4578275
        - 0.40821073
        name: normalize
        std:
        - 0.26862954
        - 0.26130258
        - 0.27577711
  init:
    callbacks:
    - ComputeNorm: {}
    - LoadCheckpointStates:
        load_checkpoint_states: model
    - KeepNCheckpoints:
        n: 1
    checkpoint:
      disable_strict_checkpoint_loading: true
      save_initial_checkpoint: false
      steps: 200
    logging:
      log_steps: 10
    loggers:
    - TensorBoardLogger:
        legacy_event_dirs: true
    loop:
      eval_steps: 1000
      max_steps: 2500
    model:
      freeze:
      - ^image_model.image_model_list
      - ^text_model
      image_model_list:
        global_image_projection:
          input_unit: 1024
          layers_activation:
          - gelu
          - null
          layers_units:
          - 4096
          - 4096
          name: FeedForwardNetwork
          use_bias: true
        image_feature_select_mode: patch
        image_models:
        - image_encoder:
            attention_dropout_rate: 0.0
            attention_softmax_fp32: true
            attention_type: scaled_dot_product
            dropout_rate: 0.0
            embedding_dropout_rate: 0.0
            filter_size: 4096
            hidden_size: 1024
            image_layer_idx: -2
            image_size:
            - 336
            - 336
            initializer_range: 0.02
            layer_norm_epsilon: 1.0e-05
            name: ViTModel
            nonlinearity: quick_gelu
            norm_first: true
            num_channels: 3
            num_heads: 16
            num_hidden_layers: 24
            patch_size:
            - 14
            - 14
            position_embedding_type: learned
            prepend_cls_token: true
            use_conv_patchified_embedding: true
            use_embed_proj_bias: false
            use_encoder_pooler_layer: false
            use_ffn_bias: true
            use_ffn_bias_in_attention: true
            use_post_embed_layer_norm: true
            use_projection_bias_in_attention: true
      loss_scaling: num_tokens
      loss_weight: 1.0
      text_model:
        attention_dropout_rate: 0.0
        attention_type: scaled_dot_product
        dropout_rate: 0.0
        embd_pdrop: 0.0
        embedding_layer_norm: false
        extra_ffn_params:
          static_dual_expert: false
        filter_size: 11008
        hidden_size: 4096
        layer_norm_epsilon: 1.0e-05
        max_position_embeddings: 2048
        name: LlamaModel
        nonlinearity: swiglu
        norm_type: rmsnorm
        num_heads: 32
        num_hidden_layers: 32
        pos_scaling_factor: 1.0
        position_embedding_type: rotary
        rotary_dim: 128
        share_embedding_weights: false
        use_bias_in_output: false
        use_ffn_bias: false
        use_ffn_bias_in_attention: false
        use_projection_bias_in_attention: false
        vocab_size: 32000
    optimizer:
      AdamW:
        betas:
        - 0.9
        - 0.999
        correct_bias: true
        eps: 1.0e-08
        weight_decay: 0.01
    precision:
      enabled: true
      fp16_type: cbfloat16
      log_loss_scale: true
      loss_scaling_factor: dynamic
      max_gradient_norm: 1.0
    schedulers:
    - SequentialLR:
        schedulers:
        - LinearLR:
            end_learning_rate: 0.001
            initial_learning_rate: 0.0
            total_iters: 100
        - CosineDecayLR:
            end_learning_rate: 5.0e-05
            initial_learning_rate: 0.001
            total_iters: 2400
    seed: 1
