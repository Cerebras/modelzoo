##############################################################
## BERT Preprocessing Parameters
##############################################################

setup:
    data:
        source: "/input/dir/here"
        type: "local"
    output_dir: "/output/dir/here"
    processes: 1
    mode: "pretraining"

processing:
    huggingface_tokenizer: <hf_tokenizer>

    max_seq_length: 512 # Maximum sequence length for BERT
    short_seq_prob: 0.0
    write_in_batch: True
    
    resume_from_checkpoint: False
    seed: 0

    read_hook: "cerebras.modelzoo.data_preparation.data_preprocessing.hooks:text_read_hook"
    read_hook_kwargs:
        text_key: "text"

    shuffle_seed: 0
    shuffle: False
    
    use_ftfy: True
    ftfy_normalizer: "NFC"
    wikitext_detokenize: False

    UNSAFE_skip_jsonl_decoding_errors: False

dataset:
    training_objective: "bert_pretraining"
    pack_sequences: False
    