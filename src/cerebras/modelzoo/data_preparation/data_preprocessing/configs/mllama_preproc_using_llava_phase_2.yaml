setup:
    data:
        source: "/cb/cold/multimodal_datasets/LLaVA/LLaVA-Instruct-150K/tiny_dataset/"
        type: "local"

    output_dir: "/cb/cold/multimodal_datasets/LLaVA/LLaVA-Instruct-150K/mllama_patches_1_img_1_preproc_tiny_dataset/"
    image_dir: "/cb/cold/multimodal_datasets/LLaVA/LLaVA-Instruct-150K/tiny_dataset/images/"
    # adjust the below to however many cores you have available
    processes: 1
    mode: "finetuning"
    
processing:
    huggingface_tokenizer: "meta-llama/Llama-3.2-11B-Vision-Instruct"
    max_seq_length: 2048

    write_in_batch: True
    write_chunk_size: 102400

    resume_from_checkpoint: False
    read_hook: "cerebras.modelzoo.data_preparation.data_preprocessing.hooks:finetuning_llava_hook"
    read_hook_kwargs:
        multi_turn_key: "conversations"
        image_key: "image"
        image_token: "<image>"
        multi_turn_content_key: "value"
        system_prompt_style: "vicuna_v1"
        phase: 2

    shuffle_seed: 0
    shuffle: True
    use_ftfy: True
    ftfy_normalizer: "NFC"
    wikitext_detokenize: False

dataset:
    num_patches: 1
    is_multimodal: True
    max_num_img: 1
    image_token: "<|image|>"